\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, geometry, fancyhdr}
\usepackage{hyperref}

% Page formatting
\geometry{a4paper, margin=1in}
\pagestyle{fancy}
\fancyhf{}
\usepackage{amsmath, amssymb, bm}

\fancyhead[L]{Methods of Statistics Notes}
\fancyhead[R]{WS 2024/2025}
\fancyfoot[C]{\thepage}

% Title, author, and date
\title{Methods of Statistics Notes}
\author{WS 2024/2025}
\date{October 2024}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\begin{document}

% Chapter 1: Basic Statistical Concepts
\section{Basic Statistical Concepts}

Consider a poll with two answers, A and B, regarding political parties. Let:
\begin{itemize}
    \item $N$: total number of voters,
    \item $M$: number of voters supporting A,
    \item $n$: size of the poll,
    \item $X_1, X_2, \ldots, X_n$: responses,
    \item Each $X_i \in \{0, 1\}$ if $X_i = 1$ supports A.
\end{itemize}

Additionally, assume:
\begin{itemize}
    \item We select $n$ individuals from $N$ at random and record their truthful reply,
    \item Every person asked replies (no selection bias),
    \item People can be asked repeatedly.
\end{itemize}

The aim of the poll is to estimate the fraction of party A supporters, say $\theta$.

\begin{definition}[Estimator]
An intuitive estimator is:
\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i
\]
\end{definition}

This estimator will be analyzed in the following sections to determine whether it is unbiased, consistent, and optimal.



% Chapter 2: Statistical Models
\section{Statistical Models}

Let $(X, \mathcal{F})$ be a measurable space, i.e., a set $X$ with a sigma-algebra $\mathcal{F}$, in which our statistical observations take values.

\begin{definition}[Statistical Model]
Let $(X, \mathcal{F})$ be some sample space. We call the parameter space $\Theta$. A statistical model is a family of probability measures $\{P_\theta\}_{\theta \in \Theta}$.
\end{definition}

\begin{remark}
Often $(X, \mathcal{F})$ is a product space. For example, if $X_i \in \{0, 1\}$, each $P_\theta$ is a product distribution, i.e., $X_1, X_2, \ldots, X_n$ are independent and identically distributed (iid). Then we say $\{P_\theta : \theta \in \Theta\}$ is an iid statistical model.
\end{remark}

\begin{remark}
If every person could only be asked once, we would have $P_\theta$ as a hypergeometric distribution, which converges to the Bernoulli model as $N, M \to \infty$.
\end{remark}



% Chapter 3: Parameter Estimation
\section{Parameter Estimation}

Assume $(\Omega, \mathcal{F}, P_\theta)$ is the setting of parametric statistics. Assume $\Theta$ is measurable.

\begin{definition}[Estimator]
An estimator for $\theta$ is any measurable function $\hat{\theta}: X \to \Theta$, i.e., any function that, based on some data $X$, outputs a guess $\hat{\theta}(X)$ for $\theta$.
\end{definition}



% Chapter 4: Unbiased and Consistent Estimators
\section{Unbiased and Consistent Estimators}

\subsection{Unbiased Estimator}
\begin{definition}[Unbiased Estimator]
Let $(\Omega, \mathcal{F}, P_\theta)$ be a measurable space. An estimator $\hat{\theta}$ is called unbiased if:
\[
\mathbb{E}[\hat{\theta}] = \theta \quad \forall \theta \in \Theta
\]
where $\mathbb{E}_{P_\theta}$ denotes expectation under the law $P_\theta$. In more explicit terms, unbiasedness means no systematic error.
\end{definition}

\begin{proof}
For the Bernoulli model, we compute:
\[
\mathbb{E}[\hat{\theta}_n] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n X_i\right] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] = \frac{1}{n} \sum_{i=1}^n \theta = \theta
\]
Thus, $\hat{\theta}_n$ is an unbiased estimator of $\theta$.
\end{proof}

\subsection{Consistent Estimator}
\begin{definition}[Consistent Estimator]
Let $\{P_{\theta, n} : n \geq 1 \}$ be a sequence of statistical models on the same parameter space. Let $\hat{\theta}_n$ be a sequence of estimators. The sequence $\hat{\theta}_n$ is called consistent if for every $\theta \in \Theta$:
\[
\hat{\theta}_n \to \theta \quad \text{in probability as} \ n \to \infty
\]
or equivalently:
\[
P_{\theta}\left(\lim_{n \to \infty} \hat{\theta}_n = \theta \right) = 1
\]
\end{definition}

\begin{proof}
For the Bernoulli model:
\[
\hat{\theta}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
We know $\mathbb{E}[\hat{\theta}_n] = \theta$ and $\text{Var}(\hat{\theta}_n) = \frac{\theta(1-\theta)}{n}$. Using Chebyshevâ€™s inequality, for any $\epsilon > 0$:
\[
P\left( |\hat{\theta}_n - \theta| > \epsilon \right) \leq \frac{\text{Var}(\hat{\theta}_n)}{\epsilon^2} = \frac{\theta(1-\theta)}{n \epsilon^2}
\]
As $n \to \infty$, this probability tends to 0, proving that $\hat{\theta}_n$ is consistent.
\end{proof}



% Chapter 5: Maximum Likelihood Estimation (MLE)
\section{Maximum Likelihood Estimation (MLE)}

\begin{definition}[Maximum Likelihood Estimator]
The maximum likelihood estimator (MLE) is the parameter that maximizes the likelihood function:
\[
L(\theta) = \prod_{i=1}^n P_\theta(X_i)
\]
\end{definition}

\subsection{Proof: MLE for Bernoulli Model}
\begin{proof}
For the Bernoulli model, $P_\theta(X_i) = \theta^{X_i}(1 - \theta)^{1 - X_i}$, so the likelihood function is:
\[
L(\theta) = \prod_{i=1}^n \theta^{X_i} (1 - \theta)^{1 - X_i} = \theta^{\sum X_i} (1 - \theta)^{n - \sum X_i}
\]
Taking the logarithm:
\[
\log L(\theta) = \sum X_i \log \theta + (n - \sum X_i) \log (1 - \theta)
\]
Setting the derivative with respect to $\theta$ equal to 0 gives:
\[
\frac{d}{d\theta} \log L(\theta) = \frac{\sum X_i}{\theta} - \frac{n - \sum X_i}{1 - \theta} = 0
\]
Solving for $\theta$, we get:
\[
\hat{\theta}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
which is the MLE.
\end{proof}



% Chapter 6: Bayesian Methods
\section{Bayesian Methods}

\begin{definition}[Posterior Distribution in Bayesian Inference]
In Bayesian statistics, a key element is the prior distribution, denoted by $\pi(\theta)$, which reflects our beliefs about the parameter $\theta$ before observing data. The posterior distribution is given by:
\[
\pi(\theta | X) \propto P_\theta(X) \pi(\theta)
\]
\end{definition}

\subsection{Example: Posterior for Bernoulli Model}
\begin{example}
Suppose we have a Beta prior for $\theta$, $\pi(\theta) \sim \text{Beta}(\alpha, \beta)$, and observe $X_1, \ldots, X_n$ as Bernoulli trials. The likelihood is:
\[
P(X | \theta) = \theta^{\sum X_i} (1 - \theta)^{n - \sum X_i}
\]
The posterior is proportional to the product of the prior and likelihood:
\[
\pi(\theta | X) \propto \theta^{\sum X_i + \alpha - 1} (1 - \theta)^{n - \sum X_i + \beta - 1}
\]
Thus, $\pi(\theta | X) \sim \text{Beta}(\sum X_i + \alpha, n - \sum X_i + \beta)$.
\end{example}

\section*{Notes on Bayes and Posterior}

\textbf{Posterior} \( = \) prior \( \times \) likelihood

\textbf{Normalizing Constant}

\[ \int \text{Posterior} \, \text{dx} = 1 \]

So,

\[
\int \text{Posterior} \, \text{dx} = 1
\]

\textbf{Prior} \( \to \) Posterior via Bayes.

Let \( \mathcal{F}_0 \) be a \(\sigma\)-algebra on \( \Omega \) and suppose \( (\Omega, \mathcal{F}_0, P_{\theta}) \) is a dominated statistical model with densities \( p(x | \theta) \). Assume

\[
x, \theta \in \Omega \quad \Rightarrow \quad p(x | \theta)
\]

is jointly measurable with respect to \( \mathcal{F}_0 \times \mathcal{F}_1 \).

Let \(\pi\) be a prior distribution on \(\Omega\) with density \( \pi(\theta) \) with respect to measure \( \nu \). Define posterior density

\[
\pi(\theta | x) = \frac{p(x | \theta) \pi(\theta)}{\int p(x | \theta) \pi(\theta) \, d\theta}
\]

The corresponding probability measure is called the \textbf{posterior distribution}.

Think of \( p(x | \theta) \) as a Lebesgue measure. Let \( \nu \) be a Lebesgue density.

\textbf{Exception}: If \(\Omega = \{0, 1\}\), then we take \(\nu\) to be the counting measure.

From the posterior, we can derive several estimators. For example, \( E[\theta | X = x] \) is convex:

\[
\int \theta p(x | \theta) \, d\theta = E[\theta | X = x]
\]

\textbf{Example:} Binomial model \( X | \theta \sim \text{Binomial}(n, \theta) \) with prior \( \theta \sim \text{Unif}(0,1) \).

For a uniform prior, we know the MAP and MLE.

Posterior mean:

\[
\theta_{\text{MAP}} = \frac{k+1}{n+2}
\]

In the case of coin flips, \( X \sim \text{Binomial}(n, \theta) \), where \( k \) is the number of heads, we conclude \( \theta | X \sim \text{Beta}(k+1, n-k+1) \).

\[
\theta | X \sim \text{Beta}(k+1, n-k+1)
\]

\textbf{Conjugate Bayes Models}: Let \( P_{\theta} \in \mathcal{P} \) be a statistical model. Then some family of priors is called \textbf{conjugate} if

\[
P_{\theta} \in \mathcal{P} \Rightarrow \theta | X \in \mathcal{P}
\]

for all \( X \in \mathcal{X} \), where \( \mathcal{X} \) is the sample space.

\[
\theta | X \sim \text{Beta}(a, b), \quad X \sim \text{Bernoulli}(p)
\]

\section*{Loss Functions and Risk}

\textbf{Loss Function}: A function \( L: \Theta \times \mathcal{X} \to [0, \infty) \) is a basis function if for every \( \theta \in \Theta \), \( L(\theta, \cdot) \) is measurable.

Given an estimator \( \delta \), the expected loss is

\[
R(\theta, \delta) = E_{\theta}[L(\theta, \delta)]
\]

\textbf{Mean Squared Error (MSE)}:

\[
L(x, y) = (x - y)^2 \Rightarrow R(\theta, \delta) = E_{\theta}[(\delta - \theta)^2]
\]

\textbf{Bias-Variance Decomposition}:

\[
L(x, y) = (x - y)^2
\]

Proof: Let \( \delta(x) = E[\theta | X = x] \).

\[
R(\theta, \delta) = E_{\theta}[(\delta(X) - \theta)^2]
\]

Bias-variance decomposition:

\[
E[(\delta(X) - \theta)^2] = \text{Var}(\delta(X)) + (\text{Bias})^2
\]

\section*{Minimax and Bayes Risk}

\textbf{Minimax Risk}: Given an estimator \( \delta \) in a model \( P_{\theta} \in \mathcal{P} \), the maximal risk of it is

\[
\sup_{\theta \in \Theta} R(\theta, \delta)
\]

The minimax of a model \( P_{\theta} \) is given as \( \inf_{\delta} \sup_{\theta} R(\theta, \delta) \), where the inf is over all estimators.

An estimator is called minimax if

\[
\sup_{\theta} R(\theta, \delta) = \inf_{\delta} \sup_{\theta} R(\theta, \delta)
\]

\textbf{Bayes Risk}: Given an estimator \( \delta \) and prior \( \pi \) on \( \Theta \), the Bayes risk of \( \delta \) is defined as

\[
R_{\pi}(\delta) = \int R(\theta, \delta) \, d\pi(\theta)
\]

The posterior risk of an estimator \( \delta(X) \) is defined by

\[
R(\delta | X = x) = E[L(\theta, \delta(X)) | X = x]
\]

Suppose \( \delta^* \) is an estimator that minimizes the posterior risk, \( \delta^*(x) = E[\theta | X = x] \). Then it also minimizes the Bayes risk.

If \( L(x, y) = (x - y)^2 \), the Bayes optimal estimator \( \delta(x) \) is the posterior mean.

We want to construct \( C(x) \) s.t. \( P_\theta (\theta \in C(x)) \geq 1 - \alpha, \, \forall \theta \in [0,1] \)

\[
x^{(1)} \quad \quad ( \quad ) \quad C(x^{(1)}) 
\]
\[
x^{(k)} \quad \quad ( \quad ) \quad C(x^{(k)}) 
\]
\[
\theta \rightarrow \quad \rightarrow \quad \rightarrow \quad \text{contains true param 3/4 times}
\]

\section*{Example cont.:}
Best guess: \( C(x) = \left[ \frac{\bar{X}_n - a}{n}, \frac{\bar{X}_n + b}{n} \right] \)

\[
P_\theta^n (\theta \in C(x)) = P_\theta^n \left( \frac{\bar{X}_n}{n} - \theta \in [-b, a] \right)
\]
\[
= F_\theta^n (a) - F_\theta^n (-b) + \rho_n
\]
where \( F_\theta^n: \mathbb{R} \to [0,1], F_\theta^n(t) = P_\theta^n \left( \frac{\bar{X}_n - \theta}{n} \leq t \right) \) is the CDF of \( \frac{\bar{X}_n - \theta}{n} \) under \( P_\theta \) and \( \rho_n = P_\theta^n \left( \frac{\bar{X}_n}{n} - \theta = -b \right) \).

\subsection*{How to choose a and b:}
\[
\text{CDF} \quad \text{CDF} \quad \leftarrow \quad -b \quad a \rightarrow t
\]
We'd like to choose \( a = (F_\theta^n)^{-1} \left( 1 - \frac{\alpha}{2} \right) \) and \( b = (F_\theta^n)^{-1} \left( \frac{\alpha}{2} \right) \), where

\[
(F_\theta^n)^{-1} (p) := \inf \{t \in \mathbb{R} : F_\theta^n(t) \geq x \} \quad \text{(Quantile Function)}
\]

Let's use a normal approximation, for \( \sigma^2 = \theta(1 - \theta) \):
\[
\sqrt{n} \left( \frac{\bar{X}_n}{n} - \theta \right) = \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{X_k - \theta}{\sigma} \overset{d}{\to} \mathcal{N}(0,1) \quad \text{[CLT]}
\]

\( X_k \sim \text{Ber}(\theta) \)

Then it follows that
\[
F_\theta^n(a_n) = P_\theta^n \left( \frac{\bar{X}_n}{n} - \theta \leq a_n \right)
\]
\[
= P_\theta^n \left( \frac{\sqrt{n}}{\sigma} \left( \frac{\bar{X}_n - \theta}{n} \right) \leq \sqrt{n} a_n \right)
\]
\[
= \Phi \left( \frac{\sqrt{n}}{\sigma} a_n \right),
\]
where the convergence is valid if \( a_n := \text{const.} \frac{1}{\sqrt{n}} \).

Now, let us choose

\[
a := \frac{\sigma}{\sqrt{n}} z_{1 - \frac{\alpha}{2}}
\]
where \( z_{1 - \frac{\alpha}{2}} = \Phi^{-1} \left( 1 - \frac{\alpha}{2} \right) \) is the \( 1 - \frac{\alpha}{2} \) quantile of \( \mathcal{N}(0,1) \) and \( b = a \). Then

\[
C(x) = \left[ \frac{\bar{X}_n}{n} - \frac{\sigma}{\sqrt{n}} z_{1 - \frac{\alpha}{2}}, \frac{\bar{X}_n}{n} + \frac{\sigma}{\sqrt{n}} z_{1 - \frac{\alpha}{2}} \right]
\]

It follows
\[
P_\theta^n (\theta \in C(x)) = F_\theta^n (a_n) - F_\theta^n (b) + \rho_n = 1 - \frac{\alpha}{2} + o(1) + o(1)
\]
\[
= 1 - \alpha + o(1) \text{ as } n \to \infty
\]

\[
\Rightarrow \text{ Asymptotically valid confidence set}
\]

One more problem: \( \sigma \) depends on \( \theta \)
\begin{itemize}
    \item Upper bound: \( \sup_{\theta \in [0,1]} \theta(1 - \theta) = \frac{1}{4} \) (maximized at \( \theta = \frac{1}{2} \))
    \item Empirical Variance: \( \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \frac{1}{n} \sum_{i=1}^n X_i)^2 \)
\end{itemize}

\[
\frac{\hat{\sigma}^2}{\sigma^2} \overset{P_\theta}{\to} 1
\]

\subsection*{Slutsky's Theorem:}
\[
X_n \overset{d}{\to} X, \quad Y_n \overset{d}{\to} \text{const.} \Rightarrow X_n Y_n \overset{d}{\to} CX
\]

\[
\text{Exercise: Use this to deduce that } a_n = \frac{\hat{\sigma}}{\sqrt{n}} z_{1 - \frac{\alpha}{2}} \text{ is also valid}
\]

\subsection*{Remark:}
% \begin{itemize}
%     \item Confidence sets have width \( o \left( \frac{1}{\sqrt{n}} \right) \)
%     \item Many confidence sets rely on limiting distributions of derived quantities such as
%     \[
%     \frac{\sqrt{n}}{\hat{\sigma}} \left( \frac{\bar{X}_n}{n} - \theta \right)
%     \]
%     \item Non-asymptotic versions require refined probability inequalities, for e.g.:\[P \left( \frac{\bar{X}_n}{n} - E \left[ \frac{\bar{X}_n}{n} \right] \leq \epsilon \right) \quad \text{[Hoeffding's inequality]}
% \end{itemize}
\section*{Hypothesis Testing}

\textbf{Definition:} Let \( (P_\theta : \theta \in \Theta) \) be a statistical model and let \( \Theta = \Theta_0 \cup \Theta_1 \) be a partition. Then:
\begin{itemize}
    \item A statistical test is a measurable function of the data \( \varphi : (\mathcal{X}, \mathcal{F}) \to [0,1] \)
    \item If \( \forall x \in \mathcal{X}, \varphi(x) \in \{0,1\} \), then \( \varphi \) is a non-randomized test
    \item Else \( \varphi \) is randomized
\end{itemize}

\subsection*{Definitions:}
\begin{itemize}
    \item \( H_0 : \theta \in \Theta_0 \) is called the null hypothesis
    \item \( H_1 : \theta \in \Theta_1 \) is called the alternative hypothesis
    \item The map \( \theta \to \beta_\varphi (\theta) = P_\theta [\varphi = 1] \) is called the power function of a test \( \varphi \)
\end{itemize}

\[
1 \quad \quad \beta_\varphi(\theta) \quad 0 \quad \quad \Theta_0 \quad \quad \Theta_1 \quad \quad \Theta
\]

\begin{itemize}
    \item For \( \theta \in \Theta_0 \), \( \beta_\varphi(\theta) \) is the type-I-error under \( \theta \) [Wrongly rejecting the null]
    \item For \( \theta \in \Theta_1 \), \( 1 - \beta_\varphi(\theta) \) is the type-II-error
\end{itemize}
\textbf{Note:}
\[
1 - P_\theta (\varphi = 1) = P_\theta (\varphi = 0) = P_\theta \text{ (wrongly accepting the null)}
\]

\textbf{Definition: [Level]} \\
\( \varphi : \mathcal{X} \to [0,1] \) has level \( \alpha \in [0,1] \) if
\[
\sup_{\theta \in \Theta_0} \beta_\varphi (\theta) \leq \alpha
\]

\textbf{Definition: [Uniformly most powerful test]} \\
Given a level \( \alpha \in (0,1) \), \( \varphi : \mathcal{X} \to [0,1] \) is called UMP if for every other test \( \varphi' \) of level \( \alpha \) and all \( \theta \in \Theta_1 \),
\[
\beta_\varphi (\theta) \geq \beta_{\varphi'}(\theta)
\]

\[
1 \quad \alpha \quad 0 \quad \quad \quad \beta_\varphi (\theta) \quad \quad \beta_{\varphi'}(\theta) \quad \quad \quad \Theta_0 \quad \quad \Theta_1
\]

\subsection*{Remark:}
In general, it is very hard to find UMP tests. But: for simple hypotheses, i.e. \( \Theta_0 = \{\theta_0\}, \Theta_1 = \{\theta_1\} \), it is possible. Here, likelihood ratio tests are UMP.

\subsection*{Theorem: [Neyman-Pearson Lemma]}
Let \( \Theta_0 = \{\theta_0\}, \Theta_1 = \{\theta_1\} \) be simple:
\begin{enumerate}
    \item \textbf{Existence:} There exists a test \( \varphi \) and a constant \( k \in [0, \infty) \), s.t. \( P_{\theta_0} (\varphi = 1) = \alpha \), of the form
    \[
    \varphi(x) = 
    \begin{cases}
      1, & \text{if } \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} > k \\
      0, & \text{if } \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} < k
    \end{cases} \quad (*)
    \]
    Here \( p_{\theta_1}, p_{\theta_0} \) are densities w.r.t. some dominated measure \( \mu \), e.g. \( \mu = p_{\theta_0} + p_{\theta_1} \). Finite \( \Theta \) implies measure is always dominated (likelihood always exists).
    
    \item \textbf{Sufficiency:} If \( \varphi \) satisfies \( P_{\theta_0} (\varphi = 1) = \alpha \) and \( (*) \) then \( \varphi \) is a UMP level \( \alpha \) test.
    
    \item \textbf{Necessity:} If \( \varphi_k \) is UMP for level \( \alpha \), then it must be of the form \( (*) \), and it also satisfies \( P_{\theta_0} (\varphi_k = 1) = \alpha \), or else it must satisfy \( P_{\theta_1} (\varphi_k = 1) = 1 \).
\end{enumerate}

\subsection*{Proof:}
\begin{enumerate}
    \item Define \( r(x) = \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} \in [0, \infty) \cup \{\pm \infty\} \). Let \( F_0 \) be the CDF of \( r(x) \) under \( P_{\theta_0} \).
    \[
    F_0(t) = P_{\theta_0} (r(x) \leq t)
    \]
    Then define also \( \alpha(t) = 1 - F_0(t) = P_{\theta_0} (r(x) > t) \)
    \begin{itemize}
        \item \( \alpha \) is right-continuous:
        \[
        \lim_{\epsilon \to 0} \alpha(t + \epsilon) = \lim_{\epsilon \to 0} P_{\theta_0} (r(x) > t + \epsilon) = P_{\theta_0} (r(x) > t) = \alpha(t)
        \]
        \item \( \alpha \) is non-increasing
        \item \( \alpha \) has left limits
        \[
        \lim_{\epsilon \to 0} \alpha(t - \epsilon) = P_{\theta_0} (r(x) > t - \epsilon) = \alpha(t^-)
        \]
    \end{itemize}

    \textbf{\( \alpha \) is cadlag:}
    \begin{itemize}
        \item Continuous from the right
        \item Limit from the left
    \end{itemize}
    \[
    \text{There exists some } k \in [0, \infty) \text{ s.t. }
    \alpha \leq \alpha(k^-) \quad \text{and} \quad \alpha \geq \alpha(k)
    \]

    We define our test
    \[
    \varphi(x) = 
    \begin{cases} 
      1 & \text{if } r(x) > k \\
      \gamma & \text{if } r(x) = k \quad \text{[reject null w.p. } \gamma] \\
      0 & \text{if } r(x) < k 
    \end{cases}
    \]
    
    We set
    \[
    \gamma = \frac{\alpha - \alpha(k)}{\alpha(k^-) - \alpha(k)}
    \]

    The level of \( \varphi \) is
    \[
    E_{\theta_0} [\varphi(x)] = P_{\theta_0} (\varphi(x) = 1)
    \]
    \[
    = P_{\theta_0} (r(x) > k) + P_{\theta_0} (r(x) = k) \cdot \gamma
    \]
    \[
    = \alpha(k) + \left[ \alpha(k^-) - \alpha(k) \right] \cdot \frac{\alpha - \alpha(k)}{\alpha(k^-) - \alpha(k)} = \alpha
    \]
    \[
    \text{(randomizing the test)}
    \]
\end{enumerate}
\section*{Lecture 6}

\subsection*{Neyman-Pearson}
\textbf{Power of a test}:
\[
E_{\theta_1}[\varphi] = P_{\theta_1}(\varphi = 1)
\]

\textbf{Likelihood ratio test}:
\[
\frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} = r(x)
\]

\subsection*{LR test}
\[
\varphi(x) = 
\begin{cases} 
1 & \text{if } r(x) > k \\
\gamma & \text{if } r(x) = k \\
0 & \text{if } r(x) < k 
\end{cases}
\]
for some \( k \in [0, \infty) \), \( \gamma \in [0, 1] \).

\textbf{Note:} LR tests are UMP for simple hypothesis testing:
\begin{itemize}
    \item Given some \( \alpha \), if LR satisfies \( E_{\theta_0}[\varphi] = \alpha \), it represents a Type I error.
    \item \( \varphi \) minimizes the Type II error
    \[
    E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi'] \quad \forall \varphi'
    \]
\end{itemize}

\subsection*{Cont. of proof (part of UMP)}
Let \( \varphi' \) be another level \( \alpha \) test, \( E_{\theta_0}[\varphi'] \leq \alpha \).

Goal: \( E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi'] \). Let \( \mu \) be the dominating measure.

Consider
\[
\int (\varphi(x) - \varphi'(x)) (p_{\theta_1}(x) - k p_{\theta_0}(x)) \, d\mu(x) = 0
\]
Claim: \( p \geq 0 \).

Observe:
\begin{itemize}
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) > 0 \Rightarrow \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} > k \Rightarrow \varphi(x) = 1 \).
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) < 0 \Rightarrow \varphi(x) = 0 \).
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) = 0 \Rightarrow \text{integrand} = 0 \).
\end{itemize}

\[
\Rightarrow p = 0
\]

\[
\Rightarrow \int (\varphi - \varphi') p_{\theta_1} \, d\mu = \int (\varphi - \varphi') p_{\theta_0} \, d\mu = k \left[ E_{\theta_0}[\varphi] - E_{\theta_0}[\varphi'] \right] \geq 0
\]

\[
\Rightarrow E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi']
\]

\textbf{Part (3) UMP} \( \Rightarrow \text{(LR)} \): Take \( \varphi^* \) a UMP test, \( E_{\theta_0}[\varphi^*] = \alpha \), and let \( \varphi \) be the LR test with \( E_{\theta_0}[\varphi] = \alpha \) with (*).

Goal: \( \varphi = \varphi^* \) a.e. except on \( \{ r(x) = k \} \).

Define
\[
x^+ = \{ x : \varphi(x) > \varphi^*(x) \}
\]
\[
x^- = \{ x : \varphi(x) < \varphi^*(x) \}
\]
\[
x^0 = \{ x : \varphi(x) = \varphi^*(x) \}
\]

\[
\tilde{x} = (x^+ \cup x^-) \cap \{ x : p_{\theta_1}(x) \neq k p_{\theta_0}(x) \}
\]

It suffices to show \( \mu(\tilde{x}) = 0 \).

Like before, we have
\[
(\varphi - \varphi^*) (p_{\theta_1} - k p_{\theta_0}) > 0 \text{ on } \tilde{x}
\]
Thus if \( \mu(\tilde{x}) > 0 \),
\[
\int_\mathcal{X} (\varphi - \varphi^*)(p_{\theta_1} - k p_{\theta_0}) \, d\mu \geq 0
\]
\[
\int_{\tilde{x}} (\varphi - \varphi^*)(p_{\theta_1} - k p_{\theta_0}) \, d\mu \geq 0
\]

But also
\[
E_{\theta_1}[\varphi] - E_{\theta_1}[\varphi^*] > k \left[ E_{\theta_0}[\varphi] - E_{\theta_0}[\varphi^*] \right] \geq 0
\]

\[
\Rightarrow \text{Cannot be } \varphi^* \text{ is UMP.}
\]

\subsection*{Example (Gaussian Location Model)}
\[
X_1, \dots, X_n \overset{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)
\]

\[
H_0 : \mu = \mu_0, \quad H_1 : \mu = \mu_1, \quad \mu_0 < \mu_1
\]

Then:
\[
\frac{p_1(X_1, \dots, X_n)}{p_0(X_1, \dots, X_n)} = \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_1)^2 + \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2 \right)
\]
\[
= \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (\mu_1^2 - \mu_0^2) - \frac{2(\mu_1 - \mu_0)}{\sigma^2} \sum_{i=1}^n X_i \right)
\]
\[
= \exp \left( -\frac{n}{2\sigma^2} (\mu_1^2 - \mu_0^2) - \frac{2(\mu_1 - \mu_0)}{\sigma^2} \sum_{i=1}^n X_i \right) \geq K_\alpha
\]

\[
\Rightarrow \frac{1}{n} \sum_{i=1}^n X_i \geq K_\alpha, \text{ some } K_\alpha \in \mathbb{R}
\]

To determine \( K_\alpha \):
\[
\bar{X}_n := \frac{1}{n} \sum X_i \overset{H_0}{\sim} \mathcal{N}(\mu_0, \sigma^2/n)
\]

\[
\Rightarrow \mathbb{L} = P_{H_0} \left( \bar{X}_n \geq K_\alpha \right) = 1 - P_{H_0} \left( \bar{X}_n < K_\alpha \right)
\]
\[
= 1 - \Phi \left( \frac{\sqrt{n}}{\sigma} (K_\alpha - \mu_0) \right) \quad \text{(CDF for } \mathcal{N}(0, 1))
\]
\[
\Rightarrow \text{solving for } K_\alpha \text{ gives }
K_\alpha = \mu_0 + \frac{\sigma}{\sqrt{n}} \Phi^{-1}(1 - \alpha),
\]
\[
\varphi(X_1, \dots, X_n) = 
\begin{cases} 
1 & \text{if } \bar{X}_n \geq \mu_0 + \frac{\sigma}{\sqrt{n}} \Phi^{-1}(1 - \alpha) \\
0 & \text{else}
\end{cases}
\]

\subsection*{Corollary}
Consider simple hypothesis testing. Let \( \varphi \) be UMP, for level \( \alpha \). Then,
\[
\alpha = E_{H_0}[\varphi_0] = E_{\theta_0}[\varphi_0] \leq E_{\theta_1}[\varphi]
\]
Suppose \( E_{\theta_1}[\varphi] = E_{\theta_1}[\varphi_0] \) then \( \varphi_0 \) is also UMP, \( \Rightarrow \varphi_0 \) is an LR test.

\[
\varphi_0 = 
\begin{cases} 
1 & \text{if } \frac{p_{\theta_1}}{p_{\theta_0}} \geq K \quad \text{a.s., some } K \\
0 & \text{if } \frac{p_{\theta_1}}{p_{\theta_0}}
\end{cases}
\]

Also since \( \varphi_0 \in \{\varphi, \beta\} \) we conclude that \( p_{\theta_1} = K p_{\theta_0} \text{ a.s.} \)

But
\[
L = \int p_{\theta_0} \, d\mu = K \int p_{\theta_0} \, d\mu = 1 \Rightarrow K = 1
\]
\textbf{Correspondence theorem}

\[
\textrm{Tests} \quad \longleftrightarrow \quad \textrm{Confidence regions } C(x)
\]

\[
\Pr_{\theta}(\theta \in C(x)) \geq 1 - \alpha
\]

\[
\textrm{If } \Pr_{\theta}(\phi_{\theta} = 1) = \alpha
\]

\textbf{Theorem:} Let $(P_\theta : \theta \in \Theta)$ be a statistical model, $\alpha \in (0, 1)$.

\begin{enumerate}
    \item[(i)] Let $C = C(X)$ be a level-$\alpha$ confidence set, then
    \[
    \phi_{\theta_0}(x) = 1 \left\{ \theta_0 \notin C(x) \right\}
    \]
    is a level-$\alpha$ test of $\theta = \theta_0$ vs. $\theta \neq \theta_0$.

    \item[(ii)] Suppose $\{\phi_{\theta_0} : \theta_0 \in \Theta \}$ is a family of level-$\alpha$ tests, then
    \[
    C(X) = \{ \theta \in \Theta : \phi_{\theta}(X) = 0 \}
    \]
    is a $(1 - \alpha)$ confidence set.
\end{enumerate}

\textbf{Proof:}

\begin{enumerate}
    \item[(i)] \quad $\Pr_{\theta_0}(\phi_{\theta_0} = 1) = \Pr_{\theta_0}(\theta_0 \notin C(X)) = \alpha$
    \item[(ii)] \quad $\Pr_{\theta}(\theta \notin C(X)) = \Pr_{\theta}(\theta \notin \{ \tilde{\theta} \in \Theta : \phi_{\tilde{\theta}}(X) = 0 \}) = \Pr_{\theta}(\phi_{\theta}(X) = 1) \leq \alpha$
\end{enumerate}

\textbf{UMPT Tests in Models with Monotone Likelihoods}

\textbf{Proposition:} Let $\Theta \subseteq \mathbb{R}$. Consider testing $H_0 : \theta \leq \theta_0$ vs. $H_1 : \theta > \theta_0$, for some $\theta_0 \in \mathbb{R}$.

Assume there exists some test statistic $T : X \to \mathbb{R}$ and a function $h : \mathbb{R} \times \Theta \times \Theta$ such that
\[
\frac{P_{\theta}(X)}{P_{\tilde{\theta}}(X)} = h(T(X), \theta, \tilde{\theta})
\]
and for all $\theta \geq \tilde{\theta}$, $t \mapsto h(t, \theta, \tilde{\theta})$ is monotone increasing.

\newpage

The simplest model for the relationship between $Y_i$ and $X_i$ assumes a linear relationship:
\[
Y_i = aX_i + b + \varepsilon_i
\]
for $i = 1, \ldots, n$, where $\varepsilon_i$ is centered, i.e., $E(\varepsilon) = 0$ and $\operatorname{Var}(\varepsilon) = \sigma^2$. Suppose $\varepsilon \sim N(0, \sigma^2)$ with $\sigma$ known.

The statistical model is given by
\[
(\mathbb{R}, B(\mathbb{R}), (\bigotimes_{i=1}^n N(ax_i + b, \sigma^2))_{(a,b) \in \mathbb{R}^2})
\]
The likelihood within the statistical model is
\[
L((a,b)|y) = \prod_{i=1}^n (2 \pi \sigma^2)^{-1/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - ax_i - b)^2 \right)
\]
The MLE satisfies the optimization problem
\[
(\hat{a}, \hat{b}) = \arg \min_{(a,b) \in \mathbb{R}^2} \sum_{i=1}^n (y_i - (ax_i + b))^2
\]
Provided that $x_i \neq x_j$ for $i \neq j$, the least squares problem has a solution with minimum given by (Gauss, 1801):
\[
(\hat{a}, \hat{b}) = \left( \frac{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}, \, \bar{y} - \hat{a} \bar{x} \right)
\]

\begin{definition}[Linear Model]
    A random vector $Y = (Y_1, \ldots, Y_n)^T \in \mathbb{R}^n$ stems from a linear model if there exists a parameter vector $\beta \in \mathbb{R}^p$, a matrix $X \in \mathbb{R}^{n \times p}$, and a random vector $\varepsilon \in \mathbb{R}^n$ such that
    \[
    Y = X \beta + \varepsilon
    \]
    \begin{enumerate}
        \item A linear model is called regular if
        \begin{enumerate}
            \item $p \le n$ (parameter size is smaller than sample size),
            \item $X$ has full rank. $rank(X) =p \le n$ (design with full rank)
            \item $E(\varepsilon)=0$ (noise is controlled)
            \item The covariance matrix is postive definate, $\Sigma = (\text{Cov}(\varepsilon_i,\varepsilon_j))_{i, j\in [n]}$
        \end{enumerate}
        \item A linear model is called ordinary if $\Sigma= \sigma^2 E_n$ (and is usually the noise is Gaussian)
    \end{enumerate}
 \end{definition}
 \begin{remark}
    \begin{enumerate}
        \item There are several synonyms
        \begin{enumerate}
            \item $Y$ a dependent variable, responce, regressand
            \item $X$, a independent variable, predictor, design matrix, regressor
            \item $\varepsilon$ Error, perturbation, reression function
        \end{enumerate}
        \item The matrix $\Sigma$ is symmetric and diagonalizable, i.e. $\Sigma = UDU^T$ for some diagonal matrix, $D = \text{diag}(\lambda_1,\hdots, \lambda_n)\in \mathbb{R}^{n\times n}$
        \item Positive semi-definate, i.e. $\lambda_i \ge 0$
        $$\langle \Sigma u, u\rangle = \langle E[(\varepsilon-E[\varepsilon])(\varepsilon-E[\varepsilon])^T]u, u \rangle $$
        $$= E[(\varepsilon-E[\varepsilon])^2]\ge 0, u\in \mathbb{R}^n$$
        item If $\Sigma$ is positve definate ($\lambda_i>0$) for $i=1,\hdots, n$, then there exists the inverse $\Sigma^{-1}=UD^{-1}U^T$ and $\Sigma^{-1/2}=UD^{-1/2}U^T.$
        \item If $X$ is not deterministic, we speak of random design.
    \end{enumerate}
 \end{remark}
In the regular linear model, $\hat{\beta}$ is called weighted least squares estimate, (LSE). if 
$$||\sigma^{-1/2}(Y-X\hat{\beta})||^2 = \inf_{\beta\in \mathbb{R}^n}||\sigma^{-1/2}(Y-X\beta)||^2 = \inf_{\beta\in \mathbb{R}^n}||\sigma^{-1/2}Y-X_\Sigma\beta||^2$$
where $X_\Sigma = \Sigma^{-1/2}X.$
$X_\Sigma \hat{\beta}$ is the point within the subspace, 
$$U = \{X_\Sigma \beta \mid \beta \in \mathbb{R}^n\}\subseteq \mathbb{R}^n$$
with the smallest distance to the vector $\Sigma^{-1/2}Y$.
Thus, $X_\Sigma \hat{\beta}= \Pi_U(\Sigma^{-1/2}Y)$ where $\Pi_U$ is the orthogonal projection onto $U$.
$\Pi_U u = u$ for all $u\in U$
$\langle \Pi_U v-v, u\rangle =0$ for all $u\in U$ and $r\in \mathbb{R}^n$.
Provided that $(X_\Sigma^T X_\Sigma)^{-1}$ exists, we can confirm by direct computation that the projection satisfies 
$$\Pi_U = X_{\Sigma}(X_\Sigma^T X_\Sigma)^{-1}X^T_\Sigma$$
For $u =X_\Sigma \beta$ we have, 
$$X_{\Sigma}(X_\Sigma^T X_\Sigma)^{-1}X^T_\Sigma X_\Sigma \beta = X_{\Sigma}\beta = u$$

By symmetry, 
$$\langle \Pi_U v-v, u\rangle =\langle v,\Pi_U u\rangle-\langle v, u\rangle=\langle v, u\rangle-\langle v, u\rangle=0$$
for all $u\in U$. 
\begin{lemma}{Representation for the LSE}
    Consider a regular linear model, then the LSE exists uniquely, and is given by 
    $$\hat{\beta}= (X^T_\Sigma X_\Sigma)^{-1}X_\Sigma^T \Sigma^{-1/2}Y = X_\Sigma^+ \Sigma^{-1/2}Y$$
    \begin{proof}
        $\ker(X_\Sigma^T X_\Sigma)$ is invertible. Suppose that $X^T_\Sigma X_\Sigma v =0$ ($v \in \ker(X_\Sigma^T X_\Sigma)$)
        $$ 0 =v^T X_\Sigma^T X_\Sigma v = (X_\Sigma^T v)^T X_\Sigma v = \langle X_\Sigma v, X_\Sigma v\rangle =||X_\Sigma v||^2= ||\Sigma^{-1/2}X v||^2 \implies ||Xv||^2 =0 \implies v=0$$
        So then 
        $$X_\Sigma \hat{\beta} =\Pi_u \Sigma^{-1/2}Y = X_\Sigma (X_\Sigma^T X_\Sigma)^{-1}X_\Sigma^T \Sigma^{-1/2}Y$$
        $$X_\Sigma^T X_\Sigma \hat{\beta} = X_\Sigma^TX_\Sigma (X_\Sigma^T X_\Sigma)^{-1}X_\Sigma^T \Sigma^{-1/2}Y$$
        $$\implies \hat{\beta}=(X_\Sigma^T X_\Sigma)^{-1}X_\Sigma^T \Sigma^{-1/2}Y$$
    \end{proof}
\end{lemma}
\begin{remark}
    \begin{enumerate}
        \item If $p>n$, then $(X_\Sigma^T X_\Sigma)^{-1}$ does not exist and the LSE is not unique. 
        $$\left\{ \beta \cdot ||\Sigma^{-1/2}Y - X_\Sigma \beta ||^2 =0\right\}$$ is a $p-n$ dim subspace and each solution interpolates the data
    \end{enumerate}
\end{remark}
\begin{theorem}{Optimality of the LSE, Gauss-Markov Theorem}
    Consider an ordinary linear model for $\sigma>0$, then 
    \begin{enumerate}
        \item The least squares estimator $\hat{\beta}= (X^TX)^{-1}X^T Y$ is linear and the unbiased parameter for the parameter $\beta$.
        \item For the desired parameter $\alpha =\langle \beta, v\rangle$ for $v \in \mathbb{R}$, the estimator $\hat{\alpha} =\langle \hat{\beta}, v\rangle$ is the best linear unbiased estimator (BLUE), 
        meaning that $\hat{\alpha}$ has the optimal value within the class of linear unbiased estimators for $\alpha$
        \item $\hat{\sigma}^2 = \frac{||Y- X\hat{\beta}||^2}{n-p}$ is an unbiased estimator of $\sigma^2$
    \end{enumerate}
    \begin{proof}
        $$\hat{\beta}(y+\tilde{y})= \hat{\beta}(y)+\hat{\beta}(\tilde{y}) \text{for }y, \tilde{y}\in \mathbb{R}^n$$
        \begin{align}
            E[\hat{\beta}]&=(X^TX)^{-1}X^T E[Y]\\
            &=(X^TX)^{-1}X^T E[X\beta +\varepsilon]\\
            &= (X^TX)^{-1}(X^TX) \beta \\
            &=\beta
        \end{align}
        Suppose that $\tilde{\alpha}$ is some other linear unbiased estimator of $\alpha$. Since the estimator is linear, there exists some element $w$ such that $\tilde{\alpha}=\langle y, w\rangle$
        $$\langle \beta, v \rangle = \alpha = E[\tilde{\alpha}]=E[\langle y, w\rangle] = \langle X\beta, w\rangle = \langle \beta, X^Tw\rangle$$
        This implies that $v=X^T w$, therefore we have, 
        \begin{align}
            \text{Var}&= \text{Var}(\langle x\beta , w\rangle+\langle \varepsilon, w\rangle)\\
            &= \text{Var}(\langle \varepsilon, w\rangle)+ E\left[\left(\sum_{i=1}^{n}\varepsilon w\right)^2\right]\\
            &=\sigma^2 \sum_{i=1}^{p} w_i^2 =\sigma^2 ||w||^2
        \end{align}
        \begin{align}
            \text{Var}(\hat{\alpha})&= E[\langle \hat{\beta}-\beta , v\rangle^2]\\
            &=E[\langle (X^T X)^{-1}X^T \beta +(X^T X)^{-1}X^T \varepsilon -\beta, v\rangle^2 ]\\
            &=E[\langle (X^T X)^{-1}X^T \varepsilon, v\rangle^2]\\
            &= \sigma^2 ||X(X^T X)^{-1} v||^2 =\sigma^2 ||X(X^T X)^{-1} X^T w||^2\\
            &=\sigma^2||\Pi_u w||^2
        \end{align}
        Thus, $\text{Var}(\hat{\alpha})\le \text{Var}\tilde{\alpha}$
    \end{proof}
\end{theorem}
\section{Lecture 8}
Recall linear model $$Y=X\beta+\varepsilon$$
where $\text{cov}(\varepsilon)=\Sigma$. 
\par OLD: $\hat{\beta} =(X_\Sigma^T X_\Sigma)^{-1}X_{\varepsilon}^T \Sigma^{-1/2}Y$.
\par $X\hat{\beta} =$ Projection of $\Sigma^{-1/2} Y$ onto span $\{X_{\varepsilon,1},\hdots, X_{\varepsilon,p}\}$
\begin{theorem}[Gauss-Markov]
    \begin{enumerate}
        \item $\hat{\beta}_{OLS}$ is the best linear unbiased est (BLUE)
        \item $\alpha_i= \langle \beta, v\rangle$ is BLUE.
        \item $\hat{\sigma}^2 =\frac{||Y-X\hat{\beta}||^2}{n-p}$ is unbiased est for $\sigma^2>0$
    \end{enumerate}
\end{theorem}
$\begin{pmatrix}  y_1  \\  y_2 \\ \vdots \\ y_n  \end{pmatrix}=\begin{pmatrix}  x_1  \\  x_2 \\ \vdots \\ x_n  \end{pmatrix} \begin{pmatrix}  \beta_1  \\  \beta_2 \\ \vdots \\ \beta_p  \end{pmatrix}^T +\begin{pmatrix}  \varepsilon_1  \\  \varepsilon_2 \\ \vdots \\ \varepsilon_n  \end{pmatrix}$
Where our data is $(Y_i, X_i)^n_{i=1}\in (\mathbb{R} \times\mathbb{R}^p)^{\otimes p}$
\begin{remark}
    Is this an iid model? Depends! 
    \begin{enumerate}
        \item Typically $\varepsilon_i$ are iid.
        \item If $X_i$ are random then "random design".
        \item If $X_i$ are iid, then linear model is iid model.
        \item If $X_i$ are deterministic, then not iid model.
    \end{enumerate}
\end{remark}
$\beta \mapsto ||Y-X\hat{\beta}||.$
\begin{proof}
    This is a continuation of point 3 in our theorem above. 
    \par We already introdiced $\Pi_U= X(X^T X)^{-1}X^T$ projection onto col space $U$ of $X$. Thus $I_n - \Pi_U$ is another projection operator, onto $U^\perp$ (othrogonal complement),
    $$U^\perp =\{z\in \mathbb{R}^n \mid \langle z, X_k\rangle \forall k=1, \hdots, p \}.$$
    Choose a basis $e_1,\hdots e_{n-p}$, orthonormal, of $U^\perp$, then 
    $$(I_n -\Pi_U)z = \Pi_{U^\perp} z = \sum_{n=1}^{n-p} \langle z, e_k \rangle e_k.$$
    \begin{align}
        ||Y-X\hat{\beta}||& = ||Y- \underbrace{X(X^TX)^{-1} X^T}_{\Pi_U} Y||^2\\
        &= ||(I_n -\Pi_n)Y||^2\\
        &= ||(I_n -\Pi_n)(X\beta + \varepsilon)||^2\\
        &= ||(I_n -\Pi_n)\varepsilon||^2\\
        &= \sum_{i=1}^{n-p} \langle \varepsilon, e_i\rangle^2 \\
    \end{align}
    Hence, $$E[||Y-X\hat{\beta}||^2\\]=\sum_{i=1}^{n-p} E[\langle \varepsilon, e_i\rangle^2] =n-p \implies E[\hat{\sigma}]=n-p$$
\end{proof}
\begin{remark}
    Recall the $N(\mu, \sigma^2)$ model, where the MLE is 
    $$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n} X_i, \quad \hat{\sigma}^2 =\frac{1}{n}\sum_{i=1}^{n} (X_i - \hat{\mu})^2.$$
    The unbiased estimator for $\sigma^2$ was $\frac{1}{n-1}\sum_{i=1}^{n} (X-i-\hat{\mu})^2$. This is related to the $n-p$ factor in point 3.
\end{remark}
\begin{remark}
    \begin{enumerate}
        \item If linearity is dropped, there exists better estimators than $\hat{\beta}_{OLS}$. For example a connstant estimator, $\hat{\beta}=\beta^*$
        \item The MSE of $\hat{\beta}_{OLS}$ is $$E[||\hat{\beta}_{OLS}- \beta ||^2] = E\left[\sum_{i=1}^{p} \langle \hat{\beta}_{OLS} -\beta, \underbrace{e_i}_{ONB of \mathbb{R}^n}\rangle^2 \right]= \sum_{i=1}^{n} \text{Var}_\beta(\langle \hat{\beta}_{OLS},e_i \rangle)=\sum_{i=1}^{n} \sigma^2 ||X(X^T X)^{-1} e_k||^2$$
        We say $X$ satisfies orthogonal design if $$X^TX = nI_p$$
        "The different covariants are uncorrelated.=" $(X^T X)_{ij}=\langle X_i, X_j\rangle =n\delta_{ij}$
        For orthogonal design, $$E_\beta[||\hat{\beta}_{OLS} -\beta||^2] = \frac{1}{n^2}\sigma^2 \sum_{i=1}^{n} \underbrace{||xe_i||^2}_n = \frac{\sigma^2 P}{n}.$$
        and this is equal to noise level times the number of parameters, divided by the number of data points. 
    \end{enumerate}
\end{remark}
\begin{theorem}[Bayes in Linear Models]
    Consider a linear model $Y= X\beta +\varepsilon$, and $\varepsilon \sim N(0, \sigma^2 I_n)$ with $\sigma>0$ known and $\beta \sim N(m,M)$ where $m \in \mathbb{R}^{p}, M \in \mathbb{R}^{p\times p}$ positive semi definate.
    Then, the posterior $\Pi(\beta| Y_iX)$ is given by 
    $$\Pi (\beta | Y,X)= N(\mu_{\text{past}}, \Sigma_\text{past})\text{ for }$$
    $$\mu_{\text{past}} = \sigma_\text{past} (\sigma^{-2}X^T y +M^{-1}m)\quad \Sigma_\text{past}= (\sigma^{-2}X^TX+M^{-2})^{-1}$$
\end{theorem}
\begin{remark}
    $\Sigma_\text{past}$ independent of $Y$. For "$M^{-2} \to 0$", then "$\mu_\text{past}\to  \hat{beta}_{OLS}$"
\end{remark}
\begin{proof}
    $$L(X,Y,\beta) \pi(\beta)\propto \exp \left(-\frac{1}{2\sigma^2}||Y-X\beta|| -\frac{1}{2}(\beta - m)^T M^{-1}(\beta-m)\right)$$
    We want this to be proportional to $\exp \left(-\frac{1}{2}(\beta - \mu_\text{past})^T \sigma_\text{past}^{-1}(\beta - \mu_\text{past})\right)$.
    \par Now, 
    $$\exp \left(-\frac{1}{2}(\beta - \mu_\text{past})^T \sigma_\text{past}^{-1}(\beta - \mu_\text{past})\right)\propto \exp \left(-\frac{1}{\sigma^2}\beta^T X^TX \beta - \frac{1}{2}\beta^T M^{-1}\beta +\frac{1}{\sigma^2}\beta^T X^TY + \beta^T M^{-1}m\right)$$
    and this is equal to $$\exp \left(-\frac{1}{2}\beta^T\left(\frac{1}{\sigma^2} X^TX +M^{-1}\right)\beta +\beta^T(\frac{1}{\sigma^2} X^TY + M^{-1}m\right)$$
    and this is $$\propto \exp \left(-\frac{1}{2}(\beta - \mu_\text{past})^T \sigma_\text{past}^{-1}(\beta - \mu_\text{past})\right)$$
\end{proof}
\begin{corollary}
    For $\ell =||\cdot ||^2$, the Bayes estimator is $\hat{\beta}_\Pi= \mu_\text{past}$
\end{corollary}
\begin{proposition}
    Consider the previous setting (from the theorem), with $m=0$, and $M=\tau^2 I_p$ (centered, isotropic, normal prior). The, $\mu_\text{past}= \hat{\beta}_\Pi$ minimizes 
    $$\beta \mapsto|| Y -X \beta ||^2_{\mathbb{R}^n}+ \underbrace{\frac{\sigma^2}{\tau^2}||\beta||^2_{\mathbb{R}^p}}_{\text{"penalty" or "regularization"}}$$
\begin{proof}
    
\end{proof}
\end{proposition}
\section{Lecture 9}

\subsection*{Proof:}
Take gradient of $\mathcal{J}(\bm{\beta})$ w.r.t. $\bm{\beta}$:
\[
\nabla_{\bm{\beta}} \mathcal{J}(\bm{\beta}) = 2 \bm{X}^\top (\bm{Y} - \bm{X} \bm{\beta}) + \frac{2\sigma^2}{\tau^2} \bm{\beta}
\]
Set $= 0$:
\[
\Rightarrow \nabla_{\bm{\beta}} \mathcal{J}(\bm{\beta}) = 2 (\bm{X}^\top \bm{X} + \frac{\sigma^2}{\tau^2} \bm{I}) \bm{\beta} - 2 \bm{X}^\top \bm{Y} = 0
\]
\[
\Rightarrow \bm{\beta} = (\bm{X}^\top \bm{X} + \frac{\sigma^2}{\tau^2} \bm{I})^{-1} \bm{X}^\top \bm{Y}
\]
Posterior mean:
\[
\mu_{\text{post}} = \bm{\Sigma}_{\text{post}}^{-1} (\bm{X}^\top \bm{Y} + \bm{M}_0^{-1} \bm{\mu}_0)
\]
\[
= (\sigma^{-2} \bm{X}^\top \bm{X} + \tau^{-2} \bm{I}_p)^{-1} \sigma^{-2} \bm{X}^\top \bm{Y}
\]
\[
= (\bm{X}^\top \bm{X} + \frac{\sigma^2}{\tau^2} \bm{I})^{-1} \bm{X}^\top \bm{Y}
\]

\subsection*{Remark:}
$\bm{\beta}$ is defined even if $\text{rank}(\bm{X}) < p$, in particular even for $n < p$.

\subsection*{Definition:}
\[
\hat{\bm{\beta}}_{\text{ridge}} = \operatorname*{argmin}_{\bm{\beta} \in \mathbb{R}^p} \|\bm{Y} - \bm{X} \bm{\beta} \|^2 + \lambda \|\bm{\beta}\|^2,
\]
is called a \textbf{Ridge Regression} estimator. Here, $\lambda > 0$ is called a regularization parameter. $\hat{\bm{\beta}}_{\text{ridge}}$ is always uniquely defined.

For $\bm{Y} = \bm{X} \bm{\beta} + \bm{\varepsilon}$, UM:
\[
\hat{\bm{\beta}}_{\text{ridge}} = (\bm{X}^\top \bm{X} + \lambda \bm{I}_p)^{-1} \bm{X}^\top \bm{Y}.
\]
Estimator independent of $\sigma^2$.

\subsection*{Proposition:}
MSE of $\hat{\bm{\beta}}_{\text{ridge}}$.

Consider a linear model with $\bm{\varepsilon} \sim N(0, \sigma^2 \bm{I}_n)$, $\sigma^2 > 0$ known, and $\bm{X}^\top \bm{X} = n \bm{I}_p$ (orthonormal design).

Let $\mathcal{J} := \langle \bm{\beta}, \bm{v} \rangle$ for $\bm{v} \in \mathbb{R}^p$, and:
\[
\delta_{\text{ridge}} = \langle \hat{\bm{\beta}}_{\text{ridge}}, \bm{v} \rangle.
\]

Then:
\begin{enumerate}
    \item 
    \[
    \mathbb{E}_{\bm{\beta}}[(\delta_{\text{ridge}} - \mathcal{J})^2] = (1 + \lambda)^{-2} \langle \bm{\beta}_v, \bm{v} \rangle^2 + \frac{\sigma^2}{n} \|\bm{v}\|^2 (1 + \lambda)^{-2}.
    \]
    \item 
    \[
    \mathbb{E}_{\bm{\beta}}[\|\hat{\bm{\beta}}_{\text{ridge}} - \bm{\beta}\|^2] = (1 + \lambda)^{-2} \|\bm{\beta}\|^2 + \frac{p \sigma^2}{n} \frac{1}{(1 + \lambda)^2}.
    \]
\end{enumerate}
We have:
\[
\hat{\bm{\beta}}_{\text{ridge}} = (\bm{X}^\top \bm{X} + \lambda \bm{I}_p)^{-1} \bm{X}^\top \bm{Y}.
\]
\[
= (\bm{n} \bm{I}_p + \lambda \bm{I}_p )^{-1} \bm{X}^\top \bm{Y}.
\]
\[
= \frac{1}{(1 + \frac{\lambda}{n})} (\bm{X}^\top \bm{X} \bm{\beta} + \bm{X}^\top \bm{\varepsilon}),
\]
where \(\bm{\varepsilon} \sim N(0, \sigma^2 \bm{I}_n)\).

\[
= \frac{1}{1 + \frac{\lambda}{n}} (\bm{X}^\top \bm{X} \bm{\beta} + \bm{X}^\top \bm{\varepsilon}),
\]
\[
= \frac{1}{1 + \frac{\lambda}{n}} \bm{\beta} + \frac{1}{1 + \frac{\lambda}{n}} \bm{X}^\top \bm{\varepsilon}.
\]

\subsection*{Bias-Variance Decomposition:}

\[
\mathbb{E}\left[(\hat{\bm{\beta}}_{\text{ridge}} - \mathcal{J})^2\right] = (\mathbb{E}[\hat{\bm{\beta}}_{\text{ridge}}] - \mathcal{J})^2 + \text{Var}(\hat{\bm{\beta}}_{\text{ridge}}).
\]
\[
= ((1 + \frac{\lambda}{n})^{-1} \langle \bm{\beta}, \bm{v} \rangle)^2 + \frac{\lambda^2}{(1 + \lambda)^2} \text{Var}(\bm{X}^\top \bm{\varepsilon}, \nu).
\]

\textbf{Observe:}
\[
(1 + \frac{\lambda}{n})^{-1} = \frac{1}{(1 + \frac{\lambda}{n})}.
\]

\textbf{Also:}
\[
\text{Var}(\bm{X}^\top \bm{\varepsilon}, \bm{\nu}) = \bm{\nu}^\top \bm{X} \text{Cov}(\bm{\varepsilon}) \bm{X}^\top \bm{\nu} = \sigma^2 \|\bm{\nu}\|^2.
\]

\subsection*{Corollary:}

Under the same assumptions:
\[
\mathbb{E}[\|\hat{\bm{\beta}}_{\text{ridge}} - \bm{\beta}\|^2] = \mathbb{E} \left[\sum_{k=1}^p (\langle \bm{\beta}, \bm{e}_k \rangle - \beta_k)^2 \right].
\]
\[
= \frac{1}{(1 + \frac{\lambda}{n})^2} \|\bm{\beta}\|^2 + \frac{p \sigma^2}{n (1 + \frac{\lambda}{n})^2}.
\]

\subsection*{Remark:}

For small \(\|\bm{\beta}\|\), Ridge $\to$ OLS. The optimal choice of $\lambda$ depends on $\|\bm{\beta}\|$.

\subsection*{1.7 Confidence Sets \& Tests in Linear Model:}

The estimators we studied are independent of \(\sigma^2\), but uncertainty quantification will depend on \(\sigma^2\)!

Assume \(\bm{\varepsilon} \sim N(0, \sigma^2 \bm{I}_n)\) throughout.

\subsubsection*{Easy Case:}

For \(\sigma^2 > 0\) known:
\[
\hat{\bm{\beta}}_{\text{OLS}} \sim N(\bm{\beta}, (\bm{X}^\top \bm{X})^{-1}).
\]

Indeed:
\[
\text{Cov}((\bm{X}^\top \bm{X})^{-1} \bm{X}^\top \bm{\varepsilon}) = (\bm{X}^\top \bm{X})^{-1}.
\]

And for \(\mathcal{J} = \langle \bm{\beta}, \bm{\nu} \rangle\),
\[
\hat{\mathcal{J}} = \langle \hat{\bm{\beta}}_{\text{OLS}}, \bm{\nu} \rangle \sim N(\mathcal{J}, \sigma^2 \bm{\nu}^\top (\bm{X}^\top \bm{X})^{-1} \bm{\nu}).
\]

Then a 95\% confidence set for \(\mathcal{J}\) is:
\[
I_{95\%}(\mathcal{J}) = \left[ \hat{\mathcal{J}} \pm 1.96 \sqrt{\bm{\nu}^\top (\bm{X}^\top \bm{X})^{-1} \bm{\nu}} \right].
\]
\subsection*{Notes on \textit{t}- and \textit{F}-distributions:}

\textbf{BUT:} Normally, $\sigma^2$ is unknown. Replace $\sigma$ by its estimator $\hat{\sigma}$.  
We need the \textit{t}- and \textit{F}-distributions.

\subsection*{Definitions:}

\textbf{Definition (t-distribution):}  
The \textit{t}-distribution with $n \geq 1$ degrees of freedom on $\mathbb{R}$ has density:
\[
f_n(x) = C_n \left( 1 + \frac{x^2}{n} \right)^{-\frac{n+1}{2}},
\]
where $C_n$ is the normalizing constant.

\textbf{Note:} For $n = 1$:
\[
f_1(x) = C_1 \frac{1}{1 + x^2},
\]
which corresponds to the \textbf{Cauchy distribution}.

\textbf{Definition (F-distribution):}  
The $F$-distribution with $(m, n) \in \mathbb{N}^2$ degrees of freedom has density:
\[
f_{m,n}(x) = C_{m,n} \frac{x^{\frac{m}{2} - 1}}{(m x + n)^{\frac{m+n}{2}}}, \quad x \in (0, \infty),
\]
where $C_{m,n}$ is the normalizing constant.

\subsection*{Why is this useful?}

\textbf{Lemma:}  
Let $X_1, \dots, X_n, Y_1, \dots, Y_n$ be i.i.d. $N(0, \Delta)$ random variables. Then:
\begin{enumerate}
    \item 
    \[
    T_n := \frac{X_n}{\sqrt{\frac{1}{n} \sum_{i=1}^n Y_i^2}} \sim t_n.
    \]
    \item 
    \[
    F_{m,n} := \frac{\frac{1}{m} \sum_{i=1}^m X_i^2}{\frac{1}{n} \sum_{j=1}^n Y_j^2} \sim F_{m,n}.
    \]
\end{enumerate}

\subsection*{Remarks:}

\begin{enumerate}
    \item 
    The \textit{t}-distribution arises when considering the "empirical mean" and "empirical variance."
    \item 
    For $n \to \infty$, $T_n \overset{d}{\to} N(0, 1)$.
\end{enumerate}

\subsection*{Proof:}

\textbf{(b) Observe:}  
\[
T^2_n = F_{1,n}.
\]
By a change of measure ($y \mapsto y^2$ in $(0, \infty)$):
\[
f_{F_{m,n}}(x) = f_{F_{m,n}}(x^2) 2x, \quad x > 0.
\]
Since $t$ is symmetric around 0, we obtain for all $x \in \mathbb{R}$:
\[
f_{T_n}(x) = f_{F_{m,n}}(x^2) |x| = C_n \left( 1 + \frac{x^2}{n} \right)^{-\frac{n+1}{2}}.
\]

It remains to show the claim for $F_{m,n}$.

Let:
\[
X = \sum_{i=1}^m X_i^2, \quad Y = \sum_{j=1}^n Y_j^2.
\]
Then:
\[
X \sim \chi^2_m, \quad Y \sim \chi^2_n,
\]
where the density of $\chi^2_m$ is:
\[
f(x) \propto x^{m/2 - 1} e^{-x/2}, \quad x > 0.
\]
\subsection*{Derivation:}

Writing \( W = \frac{X}{Y} \), we have:
\[
\mathbb{P}\left(\frac{X}{Y} < z\right) = \int_0^\infty \int_0^{zy} 1 \, f_X(x) f_Y(y) \, dx \, dy.
\]

Substituting \( x = wy \), we get:
\[
= \int_0^\infty \int_0^z 1 \, f_X(wy) f_Y(y) \, y \, dw \, dy
\]
\[
= \int_0^\infty f_X(zy) f_Y(y) \, y \, dy
\]
\[
\propto \int_0^\infty (zy)^{\frac{m}{2} - 1} y^{\frac{n}{2} - 1} e^{-(z+y)/2} \, dy.
\]

\subsubsection*{Change of Variable:}
Let \( a = \frac{z}{z+1}y \), then:
\[
\propto \int_0^\infty \left( \frac{z}{z+1} \right)^{\frac{m}{2}} a^{\frac{m}{2} - 1} e^{-\frac{z}{z+1} a} \frac{1}{z+1} da
\]
\[
\propto z^{\frac{m}{2} - 1} (z+1)^{-\frac{m+n}{2}} \int_0^\infty a^{\frac{m}{2} - 1} e^{-a} da.
\]

It follows:
\[
\frac{\partial}{\partial z} \mathbb{P}\left(\frac{X}{Y} < z\right) = f_{X,Y}(z) = \int_0^\infty f_X(zy) f_Y(y) \frac{1}{y} dy
\]
\[
\propto z^{\frac{m}{2} - 1} (z+1)^{-\frac{m+n}{2}}.
\]

\subsubsection*{Change of Variable:}
Let \( F = \frac{X}{Y} \), given \( f_F(z) = \frac{m}{n} f_{X,Y} \left( \frac{m}{n} z \right) = f_{m,n}(z). \)

\section{Lecture 10}
$t$-distribution $\cdot$ $t_n(x)\propto \left(\frac{n^2}{n}+1\right)^{-(n+1)/2}$
\par $F$ - $\cdot$ $f_{m,n}(x)\propto \frac{x^{m/2-1}}{(n+mx)^{(m+n)/2}}$
\par In the linear model $Y=X\beta+\varepsilon$, $\varepsilon\sim N(0,\sigma^2 I_n)$, $$\hat{\beta}= (X^TX)^{-1}X^TY\sim N(0, \sigma^2A)$$ where $$\sigma^2=\hat{\sigma}^2=||\frac{||Y-X\beta}{n-p}\sim \sigma^2\frac{\chi^2(n-p)}{n-p}$$
\par We now have $$t_n = \frac{N(0,1)}{\chi^2(n)n}\quad \quad f_{m,n}=\frac{n\chi^2(m)}{m\chi^2(n)}$$
\begin{lemma}
    Let $\xi\sim N(0,I_n)$, a ramdom variable in $\mathbb{R}^n$, and let $R\in \mathbb{R}^{n\times n}$ be an orthogonal projection $(R=R^2, R=R^T)$, with $\text{rank}(R)= r\le n.$
    \begin{enumerate}
        \item $\xi^T R \xi = ||R\xi ||^2\sim \chi^2(r).$
        \item If $B\in \mathbb{R}^{p\times n}$ is such that $BR =0$, then $B\xi$ is independent from $R\xi$
        \item If $S\in \mathbb{R}^{n\times n}$ is another orthogonal projection, rank$(S)=s\le n$ and $RS=0$, then $$\frac{s}{r}\frac{\xi^T R\xi}{\xi^T S\xi}\sim F(r,s)$$ 
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item Since $R$ is an othrogonal projection, there exists an orthogonal matrix $T^T=T^{-1}$ such that 
            $$R=T\begin{pmatrix} I_r & \mathbf{0}  \\
                                \mathbf{0} &   \mathbf{0}
                \end{pmatrix}T^T = TD_rT^T.$$
                Then we have $T^T\sim N(0, T^TT)= N(0,I_n)$.
                $$\xi^T R \xi=\xi^T  (TD_r T^T)\xi =(T^T \xi)^T D_r T^T \xi =\sum_{i=1}^{n} (T^T \xi )_i^2 \sim \chi^2(r).$$
                \item Let $A_1=B\xi$, $A_2=R\xi$, then $$\text{Cov}(A_1, A_2)=\text{Cov} (B\xi, R\xi)= B\text{Cov}(\xi,\xi)R^T=BR^T = BR=0$$
                \item By (2), we know $S\xi$ and $R\xi$ are independent. By (1), $\xi^TS\xi\sim \chi^2(s)$, $\xi R\xi \sim \chi^2(r)$. The claim follows from the definition of $F(r,s)$.
        \end{enumerate}
    \end{proof}
\end{lemma}
\begin{theorem}{Linear Model Confidence Sets -unknown $\sigma^2$}
    Assume regular linear model, $Y= X\beta+ \varepsilon$, rank$(X)=p\le n$, $\varepsilon\sim N(0,\sigma^2 I_n)$. Let $\alpha\in (0,1)$
    \begin{enumerate}
        \item Let $q_{F_{p, n-p},1-\alpha}$ be the $1-\alpha$ quantile of $F_{p, n-p}$ distribution. Then $C(Y,X)= \left\{\beta \in \mathbb{R}^p \mid \frac{||X (\beta-\hat{\beta}_{OLS})||^2}{p \hat{\sigma}^2}\le q_{F_{p, n-p},1-\alpha}\right\}$
        is a level $1-\alpha$ confidence set.
        \item Let $\alpha=\langle \beta, v\rangle$, for some $v\in \mathbb{R}^p$. Then a $1-\alpha$ confidence set is $$C=C(Y,X)=\left\{\alpha \in \mathbb{R} \mid \left|\frac{\alpha - \hat{\alpha}}{\hat{\sigma}\sqrt{v^T (X^TX)^{-1}v}}\right|<q\right\}$$
        where $\hat{\alpha} = \langle \hat{\beta}_{OLS},v\rangle$ and $q$ is the $1-\alpha/2$ quantile of $t_n$.  
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item We know $X\hat{\beta}_{OLS}= \Pi_U Y=\Pi_UX\beta +\Pi_U \varepsilon =X\beta +\Pi_U \varepsilon$
            Moreover, $$\hat{\sigma}^2 =\frac{||X (\beta-\hat{\beta}_{OLS})||^2}{n-p} = \frac{(||I_n-\Pi_U)Y||^2}{n-p}=\frac{||\Pi_{U^\perp}Y||^2}{n-p}=\\frac{||\Pi_{U^\perp}\varepsilon||^2}{n-p}$$
            This implies $$\frac{||X\beta -X\hat{\beta}_{OLS}}{p\hat{\sigma}^2}=\frac{|n-p|||\Pi_U\varepsilon||^2}{p||\Pi_{U^\perp}\varepsilon||^2}\sim \frac{(n-p)\sigma^2 \chi^2(p)}{p\sigma^2 \chi^2(n-p)}\sim F(p, n-p).$$
            \item We know $$\hat{\sigma}= \langle \hat{\beta}_{OLS}, v \rangle = v^T \hat{\beta}_{OLS}\sim v^T N(\beta, (X^TX)^{-1}\sigma^2)=N(\alpha, v^T (X^TX)^{-1}v \sigma^2)$$
            And this implies $$\frac{\alpha - \hat{\alpha}}{\sigma\sqrt{v^T (X^TX)^{-1}v}} \sim N(0,1).$$
            Finally, also, as in (1), $\hat{\sigma}^2 \sim \sigma^2\chi^2(n-p).$
            This implies $$\frac{\alpha - \hat{\alpha}}{\hat{\sigma}\sqrt{v^T (X^TX)^{-1}v}}\sim t_{n-p}.$$ 
        \end{enumerate}
    \end{proof}
\end{theorem}
\subsection{The $t-$ and $F-$test}
\begin{remark}[Method (t-test)]
    In a regular linear model with $\varepsilon\sim N(0,\sigma I_n)$, consider $H_0:\gamma = \gamma_0$ vs $H_1 \gamma \neq \gamma_0$ ($\gamma = \langle \beta, v\rangle$).
    The two sided $t-$test is $$\varphi_{\alpha_0}(Y,X)= \mathbf{1} (\{|T_{\alpha_0,n-p}(Y,X)|>q\}),$$
    where $$T_{\alpha_0,n-p}=\frac{\alpha_0 - \hat{\alpha}}{\hat{\sigma}\sqrt{v^T (X^TX)^{-1}v}}$$
    and $q$ is the $1-\alpha/2$-quantile of $t_{n-p}.$
\end{remark}
\begin{remark}[Method (F-test)]
    Same setting as before for $t$-test, $H_0:\beta = \beta_0$ vs $H_1 \beta \neq \beta_0$ since $\beta_0\in \mathbb{R}^p$. Then the $F$-test is 
    $$\varphi_{\beta_0}(Y,X)=\mathbb{1}(|F_{\beta_0,n-p}(Y,X)|>q)$$
    where $$F_{\beta_0,n-p}(Y,X) =\frac{||X (\beta-\hat{\beta}_{OLS})||^2}{p \hat{\sigma}^2}$$ 
    and $q=(1-\alpha)$-quantile of $F_{p,n-p}$.
\end{remark}
\subsection{General linear hypothesis testing problems}
\begin{definition}
    A linear hypothesis testomng pb. is of the form $H_0: K\beta = d$ vs $H_1: K \beta \neq d$, where $K\in \mathbb{R}^{r\times p}$ with rank$(K)=r\le p$, $d\in \mathbb{R}^p$. In other words "r linear constrations on $\beta$"
    \par $K$ os called the "contrast matrix"
    \begin{theorem}
        Assume regular linear model, with $\varepsilon \sim N(0, \sigma^2 I_n),$ and consider $H_0: K\beta =d$ vs. $K\beta\neq d$.
        \par Defin residual sum of squares as $RSS=||Y- X\beta_{OLS}||^2$ and $RSS_{H_0}=||Y- X\beta_{H_0}||^2$ and $\hat{\beta}_{H_0}$ over $\{\beta \colon K\beta =d\}$.
        \begin{proof}
            \begin{enumerate}
                \item $$\hat{\beta}_{H_0}= \hat{\beta}_{OLS}- (X^TX)^{-1} K^T(K(X^TX)^{-1}K^T)^{-1}(K\hat{\beta}_{OLS}-d)$$
                \item $RSS_{H_0}-RSS= (K \hat{\beta}_{OLS}-d)(K (X^TX)^{-1}K^T)^{-1}(K\hat{\beta}_{OLS}-d), \quad \frac{RSS_{H_0}-RSS}{\sigma^2}\sim \chi^2(r)$
                \item Define $$F=\frac{n-p}{r}=\frac{RSS_{H_0}-RSS}{RSS}=\frac{RSS_{H_0}-RSS}{r\hat{\sigma}^2}\sim F_{r,n-p}$$
                under $H_0.$
            \end{enumerate}
        \end{proof}
    \end{theorem}
\end{definition}
\section{Lecture 12}
ANOVA
\par linear model, factor/category, $F$-test for equality of means, $Y_{i,j}=\mu_i+\varepsilon_{ij}$ for $i=1,\hdots, k$ and $j=1,\hdots, k$.
\par First a note, $X^TX=||X||^2_{\mathbb{R}^n}$
\begin{theorem}
    In the ANOVA model with $\varepsilon_{ij}\sim N(0,\sigma^2)$:
    \begin{enumerate}
        \item The OLS estimate is $$\hat{\mu}=(\bar{y}_{1,\cdot },\hdots \bar{y}_{k,\cdot }) \quad \text{Recall }\bar{y}_{i,\cdot }=\frac{1}{n_i}\sum_{j=1}^{n_i} y_{ij}$$
        \item $$\frac{SSW}{\sigma^2} =\frac{1}{\sigma^2}\sum_i \sum_j (y_{ij}- \bar{y}_{i,\cdot })^2\sim \chi^2(n-k)$$
        \item Under $H_0$: $\mu_0=\mu_1=\hdots = \mu_k$, $\frac{SSB}{\sigma^2}=\frac{1}{\sigma^2}\sum_i n_i (\bar{y}_{i,\cdot} -\bar{y})\sim \chi^2(k-1)$
        \item SSW and SSB are indepdendent and under $H_0$, $$\frac{n-k}{k-1}\frac{SSB}{SSW} \sim F(k-1, n-k)$$
        \begin{proof}
            \begin{enumerate}
                \item We have $\hat{\mu}= (X^TX)^{-1}X^T Y$, with $$(X^TX)^{-1}=\begin{pmatrix}
                    \frac{1}{n_1} & \dots & 0\\
                    0 & \dots & 0\\
                    0 & \dots & \frac{1}{n_k}\\
                \end{pmatrix}$$
                and this implies that $$\hat{\mu} = \begin{pmatrix}
                    \frac{1}{n_1} & \dots & 0\\
                    0 & \dots & 0\\
                    0 & \dots & \frac{1}{n_k}\\
                \end{pmatrix} \begin{pmatrix}
                    [] & \dots & 0\\
                    0 & \dots & 0\\
                    0 & \dots & []\\
                \end{pmatrix}Y = \begin{pmatrix}
                    \frac{1}{n_1} & \dots & 0\\
                    0 & \dots & 0\\
                    0 & \dots & \frac{1}{n_k}\\
                \end{pmatrix} \begin{pmatrix}  \sum_j Y_{1,j}  \\ \vdots \\ \sum_j Y_{k,j}   \end{pmatrix}=\begin{pmatrix}  \bar{Y_{1,j}}  \\ \vdots \\ \bar{Y_{k,j}}   \end{pmatrix}$$
                \item $$SSW = ||Y-X\hat{\mu}||^2_{\mathbb{R}^n} = \sum_i \sum_j (y_j-y_{i,\cdot})^2=RSS \implies \frac{SSW}{\sigma^2}\sim \chi^2(n-k).$$
                \item We know that $SSW= RSS$ and we know that $SSW+SSB=SST = \sum_{ij}(y_{ij} -\bar{y}_{i,\cdot})^2$.
                \par We also know $\frac{RSS_{H_0}-RSS}{\sigma^2}\sim \chi^2(k-1)$ from before, it suffices to show $SST = RSS_{H_0}$, $$RSS_{H_0}=\min_{\mu \in \mathbb{R}}||Y-\mu||^2_{\mathbb{R}^2} = ||Y-\bar{Y_{\cdot \cdot}}_{\mathbb{R}^n}^2 =SST.$$
                \item Follows from general lin hypotheses testing theorem, Theorem 2.2.30 in Methoden der Statistik book.
            \end{enumerate}
        \end{proof}
    \end{enumerate}
\end{theorem}
\subsection{Exponential Families}
$$\text{General Model} (P_\theta: \theta \in \Theta) \supseteq \text{Exp. families }\supseteq \text{Linear Model}$$
Regularity Assumptions:
\par Let $(P_\theta: \theta \in \Theta)$ be a statistical model 
\begin{enumerate}
    \item Dominated, there exists $\mu$ such that $P_\theta <<\mu $ for all $\theta\in \Theta$
    \item $\Theta \in \mathbb{R}^p$ is an open set $p\ge 1$.
    \item Likelihood $p_\theta(x)>0$ for all $\theta \in \Theta, x\in X$, in particular $\log p_\theta (x)$ is well defined.
\end{enumerate}
\begin{definition}{Score}
    The score vector is $U_{\theta}(x)= \nabla_\theta \log p_\theta (x) = \begin{pmatrix}
        \frac{\partial }{\partial \theta_1}\log p_\theta (x)\\
        \vdots\\ 
        \frac{\partial }{\partial \theta_p}\log p_\theta (x)
    \end{pmatrix}$    
    whenever it exists
\end{definition}
\begin{definition}{Fisher Information}
    For $\theta \in \Theta$, the FI, whenever it exists, is $I(\theta)=E(U_\theta (x)U_\theta (x)^T)\in \mathbb{R}^{p\times p}$
\end{definition}
\par More Regularity Assumptions
\begin{enumerate}
    \item $p_\theta(x)$ is twice differentiable, in particular, $U_\theta(x)$ is well defined.
    \item $E_\theta[||U_\theta (x)||^2_{\mathbb{R}^p}]< \infty$ for all $\theta \in \Theta$, so $I(\theta)$ is well defined. 
    \item $$\int h(x) \nabla_\theta p_\theta(x) \mu (dx)= \nabla_\theta \int h(x)$$ for relevant $h(x)$.
\end{document}