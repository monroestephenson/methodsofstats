% Für Hausaufgaben article, für Skripte report
% \documentclass[open=any, 11pt,paper=A4,headings=big,titlepage = firstiscover]{scrreprt}
\documentclass[open=any, 11pt,paper=A4]{scrreprt}


\KOMAoptions{}


  
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage[
backend=biber,
style=alphabetic, % Label wird verschönert
sorting=ynt,
backrefstyle=two % sortiert nach year, name und dann title
]{biblatex}
\usepackage[textsize=tiny,linecolor=black,backgroundcolor=white]{todonotes}
\usepackage{0_einstellungen}
\addbibresource{0_bibliography.bib} %Imports bibliography file
\usepackage[font=footnotesize]{caption}
\usepackage{wrapfig}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{pdfpages,datetime}


% \author{Said Kassner}
% \title{Lecture Notes}

% MAKROS
%%%%%%%%%%%%%%%%%
\input{0_macros} 
%%%%%%%%%%%%%%%%%

% In den folgenden drei Zeilen sorgst du dafür dass es kein Problem mehr damit gibt dass du auf der Titelseite \sc benutzt
\makeatletter
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother
\makeatletter
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\@nomath\it}
\makeatother

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%


\begin{document}
% \includepdf{0_title.pdf}
\input{0_titlepage} % Kann man auch mit include machen, so war es ursprünglich
%\newpage\null\thispagestyle{empty}\newpage % leere Seite nach dem Titelblatt



\tableofcontents % Inhaltsverzeichnis direkt nach der Title Page
\thispagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% REWORKED CODE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Intro and Disclaimer}\label{ch:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

These lecture notes are based on the material presented by Professor Wang during class and are written by students. They may contain errors or omissions. Please refer to the in-person lectures and the literature on Moodle for accurate and authoritative information. 
If you find an error or want to help, please send an email to:
\begin{center}
    \href{mailto:said.kassner@student.hu-berlin.de}{said.kassner@student.hu-berlin.de}
\end{center}

\begin{center}
    \href{mailto:stephensonmonroe@gmail.com}{stephensonmonroe@gmail.com}
\end{center}

\begin{center}
    \href{mailto:salihiad@hu-berlin.de}{salihiad@hu-berlin.de}
\end{center}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Basic Statistical Concepts}\label{ch:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Lecture 1}
Here is the literature that the class is based on.

\textbf{Literature:}
\begin{itemize}
    \item WS 19/20 R. Altmeyer \textit{"Gliederung Methoden der Statistik"}
    \item L. Wasserman, \textit{All of Statistics}
    \item M. Trabs, K. Krenz, M. Jirak and M. Reiss. \textit{Methoden der Statistik.}
    \item Hastie, Tibshirani, et al., \textit{Elements of Statistical Learning}
\end{itemize}

Let’s start with a (simplest possible) example:

\begin{example}[Polling]\label{ex:polling}
Consider a poll with two answers A and B (representing political parties).
\begin{itemize}
    \item \( N \) = total number of votes
    \item \( M \) = total number of votes supporting party A
\end{itemize}
\end{example}

\textbf{Poll Definitions:}
\begin{itemize}
    \item \( n \) = size of the poll
    \item \( x = (x_1, ..., x_n) \) = responses, where:
    \[
    x_i =
    \begin{cases} 
    0 & \text{if the i\text{-}th person supports B} \\
    1 & \text{if the i\text{-}th person supports A}
    \end{cases}
    \]
\end{itemize}

\textbf{Additional Assumptions:}
\begin{itemize}
    \item \( n \)-times, we select a person randomly from the set \( \{1, ..., N\} \), and record their (truthful) response.
    \item Every asked person responds (i.e., no selection bias).
    \item People can be asked repeatedly.
\end{itemize}

\textbf{Aim of the Poll:} The aim of the poll is to estimate the fraction of party A supporters. This can be written as:

\[
\theta = \frac{M}{N} \in [0, 1]
\]

An intuitive estimate of \( \theta \) is:

\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_i
\]

\textbf{Question:} Is this a good (or best possible) estimator? What properties does it have?

To answer this, we formalize some statistical notions.

\begin{definition}[Sample space]\label{def:samplespace}
A \textit{sample space} is a measurable space \((\mathcal{X}, \mathcal{F})\), i.e., a set \(\mathcal{X}\) with a \(\sigma\)-algebra \(\mathcal{F}\), in which our statistical observations take values.
\end{definition}

\begin{definition}[Statistical model]\label{def:statmodel}
Let \((\mathcal{X}, \mathcal{F})\) be some sample space and let \(\Theta\) be a set, which we call the \textit{parameter space}. A \textit{statistical model} is a family of probability measures \(\{P_\theta : \theta \in \Theta\}\) on \((\mathcal{X}, \mathcal{F})\).
\end{definition}

\begin{remark}
Often, \((\mathcal{X}, \mathcal{F})\) is a "product space." For example, in Example \ref{ex:polling}, \(\mathcal{X} = \{0, 1\}^n\), and each \(P_\theta\) is a product distribution, i.e., \(x_1, \dots, x_n\) are independent, identically distributed. Then we say \(\{P_\theta : \theta \in \Theta\}\) is an \textit{iid statistical model}.
\end{remark}

\begin{remark}[Back to Example \ref{ex:polling}]
Here:
\begin{itemize}
    \item \(\mathcal{X} = \{0, 1\}^n\)
    \item \(\Theta = [0, 1]\)
    \item \(\mathcal{F} = \mathcal{P}(\{0, 1\}^n)\)
    \item \(P_\theta = (\text{Bernoulli}(\theta))^{\otimes n}\)
\end{itemize}
\end{remark}

\begin{remark}
If every person could only be asked once, we would have \( P_\theta = \text{Hypergeometric}(N, M, n) \), which "converges" to the Bernoulli model as \( N, M \to \infty \). 
We might have to discretize \(\Theta\) and take \( \theta = \frac{M}{N} \). 
(Exercise: Think about it!)
\end{remark}

\chapter{Parameter Estimation}
Assume that \( \Theta \subseteq \mathbb{R}^p \), for \( p \geq 1 \). This is the setting of parametric statistics. 
[Assume \(\Theta\) is measurable.]

\begin{definition}[Estimator]\label{def:estimator}
An estimator for \( \theta \in \Theta \) is any measurable function:
\[
\hat{\theta}: (\mathcal{X}, \mathcal{F}) \to \Theta.
\]
Any function that, based on some data \( x \in \mathcal{X} \), outputs a guess / estimate \( \hat{\theta}(x) \in \Theta \).
\end{definition}


\section*{Lecture 2}

\textbf{Last time:} Statistical model = family of probability measures on $(\mathcal{X}, \mathcal{F})$ indexed by $\theta \in \Theta$. 

\textbf{Sample space:} $(\mathcal{X}, \mathcal{F})$

\textbf{Estimator:} = measurable function $(\mathcal{X}, \mathcal{F}) \to \Theta$

Now, what are some desirable properties we would like to have?

\begin{definition}[Unbiased estimator]\label{def:unbiased}
Let $\Theta = \mathbb{R}^p$ (measurable), $p \geq 1$. An estimator $\hat{\theta}$ is unbiased if 
\[
    \mathbb{E}_{\theta}[\hat{\theta}] = \mathbb{E}_{\mathbb{P}_\theta}[\hat{\theta}] = \theta, \text{ for all } \theta \in \Theta.
\]
Where $\mathbb{E}_{\theta}[\cdot] = \mathbb{E}_{\mathbb{P}_\theta}[\cdot]$ denotes expectation under the law $\mathbb{P}_\theta$.
\end{definition}


In more explicit terms:
\[
    \mathbb{E}_{x \sim \mathbb{P}_\theta}[\hat{\theta}(x)] = \theta \quad \forall \theta
\]

\begin{remark}[Unbiasedness]\label{rem:unbiasedness}
Unbiasedness means "no systematic errors." However, we'd also like a "good" $\hat{\theta}$ to be concentrated around the data-generating parameter.
\end{remark}

\begin{definition}[Consistent estimator]\label{def:consistent}
Let $(\mathbb{P}_{\theta}^n)_{\theta \in \Theta}$ be a sequence of statistical models ($n \geq 1$), on the same parameter space $\Theta$ not depending on $n \geq 1$.

Let $\hat{\theta}_n$ be a sequence of estimators. Then $\hat{\theta}_n$ is called consistent if for every $\theta \in \Theta$,
\[
    \hat{\theta}_n \xrightarrow{\mathbb{P}_\theta^n} \theta
\]
or explicitly, for every $\epsilon > 0$,
\[
    \lim_{n \to \infty} \mathbb{P}^n_\theta (|\hat{\theta}_n - \theta| > \epsilon) = 0.
\]
\end{definition}

\textbf{Back to Example \ref{ex:polling}:}
\begin{itemize}
    \item $X_i = \{0, 1\}^n$
    \item $\Theta = [0, 1]$
    \item $\mathbb{P}_\theta^n = \text{Bernoulli}(\theta)^{\otimes n}$
    \item $\hat{\theta}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$
\end{itemize}

\textbf{Unbiasedness:} \\
Let $\theta \in \Theta$, then
\[
    \mathbb{E}_\theta \left[ \frac{1}{n} \sum_{i=1}^{n} X_i \right] = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_\theta [X_i] = \frac{1}{n} \sum_{i=1}^{n} \theta = \theta.
\]
Thus, $\mathbb{E}_\theta [\hat{\theta}_n] = \theta$.

\textbf{Consistency:}
\begin{itemize}
    \item We could use the Weak Law of Large Numbers (WLLN).
    \item Alternatively,
\[
    \text{Var}_\theta(\hat{\theta}_n) = \frac{1}{n^2} \sum_{i=1}^{n} \text{Var}_\theta (X_i) = \frac{1}{n^2} \sum_{i=1}^{n} \theta (1-\theta) = \frac{\theta(1-\theta)}{n}
\]
which tends to zero as $n \to \infty$.
\end{itemize}

It follows: For every $\epsilon > 0$,
\[
    \mathbb{P}^n_\theta(|\hat{\theta}_n - \theta| > \epsilon) = \mathbb{P}^n_\theta(|\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]| > \epsilon)
\]
By Markov's inequality:
\[
    \mathbb{P}^n_\theta(|\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]| > \epsilon) \leq \frac{\mathbb{E}_\theta \left[ (\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n])^2 \right]}{\epsilon^2} = \frac{\text{Var}_\theta (\hat{\theta}_n)}{\epsilon^2} = \frac{\theta(1-\theta)}{n \epsilon^2}
\]
which tends to zero as $n \to \infty$. Thus,
\[
    (\hat{\theta}_n : n \geq 1) \text{ is consistent. } \quad \square
\]

\section{Maximum Likelihood Principle}

Is there another way to motivate $\hat{\theta}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$? Yes, it turns out it is the maximum likelihood estimator, i.e.,
\[
    \text{MLE} = \text{"parameter which assigns the highest probability to the observed data."}
\]

\textbf{In our example}, each $\mathbb{P}_\theta^n$ has a probability density (likelihood)
\[
    \mathbb{P}_\theta^n(x) = \prod_{i=1}^{n} \mathbb{P}_\theta(x_i) = \prod_{i=1}^{n} \theta^{x_i} (1-\theta)^{1-x_i}
\]
\[
    = \theta^{\sum_{i=1}^{n} x_i} (1-\theta)^{n-\sum_{i=1}^{n} x_i}.
\]

Fixing $x \in \{0,1\}^{n}$ and maximizing in $\theta \in [0,1]$ gives the following:

\begin{itemize}
    \item If $\sum_{i=1}^{n} x_i = 0$, then $\hat{\theta}_n = 0$ is the maximizer.
    \item If $\sum_{i=1}^{n} x_i = n$, then $\hat{\theta}_n = 1$ is the maximizer.
    \item If $\hat{\theta}_n \in \{1, \ldots, n-1\}$, then writing $S_n = \sum_{i=1}^{n} x_i$ gives:
\end{itemize}

\[
    \frac{\partial}{\partial \theta} \mathbb{P}_\theta^n(x) = S_n \theta^{S_n-1} (1-\theta)^{n-S_n-1} - (n-S_n)\theta^{S_n} (1-\theta)^{n-S_n-1} = 0
\]
\[
    \Leftrightarrow S_n (1 - \theta) - \theta (n - S_n)
\]
\[
    \Leftrightarrow \theta = \frac{S_n}{n}.
    \quad \square
\]

\begin{definition}[Dominated statistical model \& MLE]\label{def:dominatedmodel}
A model $(\mathbb{P}_\theta^n)_{\theta \in \Theta}$ is called dominated if there exists a measure $\mu$ on $(\mathcal{X}, \mathcal{F})$ such that for every $\theta \in \Theta$, $\mathbb{P}_\theta \ll \mu$ or equivalently (by Radon-Nikodym), for all $\theta \in \Theta$, there is a probability density $\frac{\mathrm{d} \mathbb{P}_\theta}{\mathrm{d}\mu}$ of $\mathbb{P}_\theta$ with respect to $\mu$.

The MLE is defined as any $\hat{\theta} \in \Theta$ that maximizes the function
\[
    \theta \mapsto \frac{\mathrm{d} \mathbb{P}_\theta}{\mathrm{d}\mu}(x) = \mathbb{P}_\theta(x).
\]
\end{definition}

\begin{remark}[Caveats]\label{rem:caveats}
\begin{itemize}
    \item MLE might not be unique.
    \item MLE might not exist.
    \item It's not always clear that some selection $\hat{\theta}(x) \in \arg \max_{\theta} \mathbb{P}_\theta(x)$ is a measurable function of $x \in \mathcal{X}$. However, there are measurable selection theorems that permit a measurable choice of $\hat{\theta}$ under very general conditions.
\end{itemize}
\end{remark}

\begin{remark}\label{rem:mu}
In all the models we study, we will work with the Lebesgue measure (for continuous data) or the counting measure (for discrete data).
\end{remark}

\begin{example}[Normal model]\label{ex:normalmodel}
Consider random samples $X_1, \dots, X_n \sim N(\mu, \sigma^2)$ for some unknown $\mu \in \mathbb{R}$, $\sigma^2 > 0$, and let $\lambda$ denote the Lebesgue measure on $\mathbb{R}^n$.

\[
    \theta = (\mu, \sigma^2) \in \mathbb{R} \times (0, \infty).
\]
Then the likelihood is:
\[
    L(\mu, \sigma^2 \mid x) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{1}{2} \left( \frac{X_i - \mu}{\sigma} \right)^2 \right)
\]
\[
    = (2 \pi \sigma^2)^{-n/2} \exp \left( - \frac{1}{2 \sigma^2} \norm{X-\mu\cdot 1_n}^2 \right),
\]
where by $1_n$ we denote the vector of ones of dimension $n$.

Here, the MLE is given as:
\[
    \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \text{[Sample mean]}, \quad \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \hat{\mu})^2 \quad  \text{[Sample variance]}.
\]
\[
    \mathbb{E}_\theta[\hat{\mu}] = \mu, \quad \mathbb{E}_\theta[\hat{\sigma}^2] = \frac{n-1}{n} \sigma^2, \quad \text{so } \hat{\sigma}^2 \text{ is biased.}
\]
\[
    \Rightarrow \text{MLE is not always unbiased, and not always a "good" method.}
\]

\end{example}

\section{Bayesian method}

\paragraph{Motivation}
In Bayesian statistics, a key element is the prior distribution, which we denote by $\pi$, reflecting our "beliefs" about the parameter $\theta \in \Theta$ before observing data ($\pi$ is a probability measure on $\Theta$).

A prior $\pi$, together with a model $(P_\theta : \theta \in \Theta)$, gives rise to a joint probability distribution for the pair $(\theta, x) \in \Theta \times \mathcal{X}$.

The Bayesian approach bases statistical inference on the posterior distribution of $\theta$ conditioned on $x$.

\textbf{Joint probability:}
\[
    (\theta, x) \mapsto \pi(\theta) P_\theta(x)
\]
\hspace{4cm} \text{conditional distribution of } $x \mid \theta$

\textbf{Posterior:}
\[
    \pi(\theta \mid x) = \frac{\pi(\theta) P_\theta(x)}{\int_{\Theta} \pi(\theta) P_\theta(x) \, d\theta}
\]

\begin{remark}
Bayesian methods automatically generate "error bars" because the posterior is not an estimator but a whole probability distribution.
\end{remark}



\section*{Lecture 3}

\begin{definition}[Prior, Posterior, Bayes' Rule]\label{def:prior, posterior, bayes rule}
Let $\mathcal{F}_\Theta$ be a $\sigma$-algebra on $\Theta$, and suppose 
\[
\left\{ P_\theta : \theta \in \Theta \right\}
\]
is a dominated statistical model with densities $p_\theta(x)$, and assume that 
\[
(\theta, x) \mapsto p_\theta(x)
\]
is "jointly measurable" (i.e., w.r.t. $\sigma(\mathcal{F}_\Theta \times \mathcal{F})$).

Let $\Pi$ be a prior distribution on $\Theta$, with density $\pi(\theta)$ w.r.t. measure $\nu(\cdot)$. Then, define the posterior density
\[
\pi(\theta \mid x) := \frac{p_\theta(x) \pi(\theta)}{\int_\Theta p_{\tilde{\theta}}(x) \, d\Pi(\tilde{\theta})}.
\]
\end{definition}

The corresponding probability measure $\Pi(\cdot \mid x)$ is called the posterior distribution:
\[
\Pi(B \mid x) = \int_B \pi(\theta \mid x) \, d\nu(\theta), \quad B \in \mathcal{F}_\Theta.
\]
\[
= \frac{\int_B p_\theta(x) \, \pi(\theta) \, d\nu(\theta)}{\int_\Theta p_{\tilde{\theta}}(x) \, d\Pi(\tilde{\theta})},
\]
\[
= \frac{\int_B p_\theta(x) \, d\Pi(\theta)}{\int_\Theta p_{\tilde{\theta}}(x) \, d\Pi(\tilde{\theta})},
\]

\begin{remark}
Think of $\Theta \subseteq \mathbb{R}^p$, $\nu(\cdot)$ as a Lebesgue measure, $\pi(\cdot)$ as a Lebesgue density.

\textbf{Exception:} $\Theta = \{0,1\}$ in hypothesis testing. Then, we'd take $\nu(\cdot)$ to be the counting measure.
\end{remark}

From the posterior, we can derive several estimators:
\begin{itemize}
    \item \textbf{Maximum-a-posterior (MAP) estimator:}
    \[
    \hat{\theta}_{\text{MAP}}(x) = \underset{\theta \in \Theta}{\text{argmax}} \, \pi(\theta \mid x).
    \]
    \item \textbf{Posterior mean:} Say \( \Theta \subseteq \mathbb{R}^p \) convex
    \[
    \hat{\theta}(x) = \int_\Theta \theta \, \pi(\theta \mid x) \, d\nu(\theta) \in \mathbb{R}^p.
    \]
\end{itemize}

\textbf{Back to Example 1.1:}  
\textit{Binomial model:} $\mathcal{X} = \{0, 1, \dots, n\}$,  
$p_\theta = \text{Bin}(n, \theta)$,  
$\theta \in \Theta = [0,1]$.  

\textit{Prior (uniform):} $\Pi = \text{Unif}(0,1)$.

We know: 
\[
\hat{\theta}_{\text{MAP}} = \hat{\theta}_{\text{MLE}} \quad \text{(for the uniform prior)},
\]
\[
\hat{\theta}_{\text{MAP}} = \frac{X}{n}.
\]

\begin{itemize}
    \item \textbf{Posterior mean}:
    \[
    \pi(\theta | x) = \frac{p_\theta(x)}
    {\int p_{\tilde{\theta}}(x) d\tilde{\theta}} \propto  \binom{n}{k} \theta^{x}(1-\theta)^{n-x}.
    \]
    
    \item \textbf{Binomial distribution}:
    \[
    \text{Bin}(n, p)(k) = \binom{n}{k} p^k (1-p)^{n-k},
    \]
    where \( k \in \{0, \ldots, n\} \) is the number of successes, and \( p \) is the probability of success, and $n$ would be interpreted as the "number of coin flips".
\end{itemize}

\[
\pi(\theta | x) \propto \theta^x (1-\theta)^{n-x}.
\]
and 

\[ \int_0^1 \pi(\theta \mid x) d\nu  =1\]

We conclude that \( \pi(\theta | x) \) is a \textbf{Beta-distribution} on \( [0,1] \),
\[
\text{Beta}(x+1, n-x+1).
\]

The mean is given by:
\[
\hat{\theta} = \frac{x+1}{n+2}.
\]

\begin{remark}[Beta distribution]
The Beta distribution is defined as:
\[
\text{Beta}(a, b), \quad a,b \geq 0.
\]
The probability density function of the Beta distribution is given by:
\[
P_{\text{Beta}(a,b)}(x) = x^{a}(1-x)^{b}.
\]
\end{remark}

\begin{definition}[Conjugate Bayesian models]
Let \((P_\theta : \theta \in \Theta)\) be a statistical model. Then, some family \(\mathcal{D}\) of p.m.s on \(\Theta\) is called \textit{conjugate} if
\[
\Pi \in \mathcal{D} \implies \Pi(\cdot | x) \in \mathcal{D} \quad \text{for all } x \in \mathcal{X}.
\]
\end{definition}

\noindent \textbf{Examples:}
\begin{itemize}
    \item \((\text{Bin}(n, \theta)) : \theta \in [0,1], \quad \mathcal{D} = \text{Beta}(a,b), \ a,b \geq 0.\)
    \item \((\mathcal{N}(\mu, \sigma^2)) : \mu \in \mathbb{R}, \quad \mathcal{D} = \left\{\mathcal{N}\left(\mu, n^2\right), \mu \in \mathbb{R}, n^2 > 0 \right\}, \ \sigma^2 \)\text{ known}.
\end{itemize}


\chapter{Decision Theory}

Here suppose that \(\Theta \subseteq \mathbb{R}^p\).
\begin{definition}[Loss function]
A function \(\ell : \Theta \times \mathbb{R}^p \to [0, \infty)\) is a \textit{loss function} if for every \(\theta \in \Theta\), \(\ell(\theta, \cdot)\) is measurable. Given some estimator \(\hat{\theta}\), the expected loss is
\[
\mathcal{R}(\theta, \hat{\theta}) = \mathbb{E}_\theta \left[\ell(\theta, \hat{\theta})\right].
\]
\end{definition}

\noindent \textbf{Example:} Take \(\ell(\theta, \hat{\theta}) = \| \theta - \hat{\theta} \|^2_{\mathbb{R}^p}.\) Then,
\[
\mathcal{R}(\theta, \hat{\theta}) = \mathbb{E}_\theta \left[\|\theta - \hat{\theta}\|^2_{\mathbb{R}^p} \right]
\]
is the mean squared error (MSE).

\begin{proposition}[Bias-Variance Decomposition]
    Let $\hat{\theta} \in L^2(\mathbb{P}_\theta)$. Then it holds that:
    \[
    R(\hat{\theta}, \theta) = (\mathbb{E}_\theta[\hat{\theta}] - \theta)^2 + \operatorname{Var}_\theta(\hat{\theta}).
    \]
\end{proposition}

\begin{proof}
    We have
    \[
    R(\hat{\theta}, \theta) = \mathbb{E}_\theta[(\hat{\theta} - \mathbb{E}_\theta[\hat{\theta}] + \mathbb{E}_\theta[\hat{\theta}] - \theta)^2]
    \]
    Expanding the squared term:
    \[
    = \mathbb{E}_\theta[(\hat{\theta} - \mathbb{E}_\theta[\hat{\theta}])^2] + (\mathbb{E}_\theta[\hat{\theta}] - \theta)^2 + 2\mathbb{E}_\theta[(\hat{\theta} - \mathbb{E}_\theta[\hat{\theta}])(\mathbb{E}_\theta[\hat{\theta}] - \theta)].
    \]
    Since $\mathbb{E}_\theta[\hat{\theta} - \mathbb{E}_\theta[\hat{\theta}]] = 0$, the last term vanishes, leaving us with:
    \[
    R(\hat{\theta}, \theta) = \operatorname{Var}_\theta(\hat{\theta}) + (\mathbb{E}_\theta[\hat{\theta}] - \theta)^2.
    \]
\end{proof}

\begin{definition}[Minimax Risk]
    Given an estimator $\hat{\theta}$ in a model $(\mathbb{P}_\theta : \theta \in \Theta)$, the maximal risk of $\hat{\theta}$ is
    \[
    \sup_{\theta \in \Theta} R(\theta, \hat{\theta}).
    \]
    The minimax risk of a model $(\mathbb{P}_\theta : \theta \in \Theta)$ is given as
    \[
    \inf_{\hat{\theta}} \sup_{\theta \in \Theta} R(\theta, \hat{\theta}),
    \]
    where the infimum is taken over all estimators. An estimator is called minimax if
    \[
    \sup_{\theta \in \Theta} R(\theta, \hat{\theta}) = \inf_{\hat{\theta}} \sup_{\theta \in \Theta} R(\theta, \hat{\theta}).
    \]
\end{definition}

\begin{definition}[Bayes Risk]
    Given a prior $\pi$ on $\Theta$, the $\pi$-Bayes risk of a decision rule $\delta$ for the loss function $L$ is defined as
    \[
    R_\Pi(\delta) = \mathbb{E}_\Pi [R(\delta, \theta)] = \int_{\Theta} R(\delta, \theta) \pi(\theta) d\theta = \int_{\Theta} \int_{\mathcal{X}} L(\delta(x), \theta) \pi(\theta) p_\theta(x) dx d\theta.
    \]
    A $\Pi$-Bayes decision rule $\hat\theta_\Pi$ is any decision rule that minimizes $R_\Pi(\hat\theta)$.
\end{definition}

\textcolor{red}{SW: $\ell$ instead of $L$ below, $p_{\theta}(x)$ instaed of  $f(x, \theta)$ }
\textcolor{green}{Note: Has been corrected. }

\begin{definition}[Posterior Risk]
    For a Bayesian model, the posterior risk $R_{\Pi}$ is defined as the average loss under the posterior distribution for some observation $x \in \mathcal{X}$:
    \[
    R_{\Pi(\cdot|x)}(\delta) = \mathbb{E}_\Pi[\ell(\delta(x), \theta) | x].
    \]
    Here, the notation $\mathbb E_\Pi[\cdot|x]$ stands for the expectation under the posterior distribution.
\end{definition}

\begin{proposition}[Bayes Risk and Posterior Risk]
    An estimator $\delta$ that minimizes the $\Pi$-posterior risk $R_{\Pi}$ also minimizes the $\pi$-Bayes risk $R_\pi$.
\end{proposition}

\begin{proof}
    The $\pi$-Bayes risk can be rewritten as
\begin{align*}
    R_\pi(\delta) &= \int_{\Theta} \mathbb{E}_\theta[\ell(\delta(X), \theta)] \pi(\theta) d\theta\\
    &= \int_{\Theta} \int_{\mathcal{X}} \ell(\delta(x), \theta) \pi(\theta) p_{\theta}(x) dx d\theta\\
    &= \int_{\mathcal{X}} \int_{\Theta} \ell(\delta(x), \theta) \frac{p_{\theta}(x) \pi(\theta)}{\int_{\Theta} p_{\theta'}(x) \pi(\theta') d\theta'} \times \underbrace{\int_{\Theta} p_{\theta'}(x) \pi(\theta') d\theta'}_{=: n(x) \geq 0} dx d\theta\\
    &= \int_{\mathcal{X}} \mathbb{E}_{\Pi}[\ell(\delta(x), \theta) | x] n(x) dx.
\end{align*}
    [Notation $n(x)$ motivated by the word `normalising constant'].

    
    Let $\delta_\Pi$ be a decision rule that minimizes the posterior risk, i.e., such that for all $x \in \mathcal{X}$,
    \[
    \mathbb{E}_{\Pi}[\ell(\delta_\Pi(x), \theta) | x] \leq \mathbb{E}_{\Pi}[\ell(\delta(x), \theta) | x].
    \]
    Multiplying by $m(x) \geq 0$ and integrating on both sides over $\mathcal{X}$ yields the desired result.
\end{proof}

\begin{example}
    For the quadratic risk with the squared-loss, the posterior risk is minimized by taking $\delta(X) = \mathbb{E}_{\Pi}[\theta | X]$, by minimizing the quadratic function in $\delta$. Other losses will give other ways to minimize the posterior risk, and other Bayes decision rules.
\end{example}

\begin{proposition}
Let \((\mathcal{X}, \mathcal{F}, (\mathbb{P}_{\theta})_{\theta \in \Theta})\) be a statistical model and let \(\hat{\theta}\) be an estimator. Then we have
\[
\sup_{\theta \in \Theta} R(\hat{\theta}, \theta) = \sup_{\Pi} \int_{\Theta} R(\hat{\theta}, \theta) \, \Pi(d\theta),
\]
where the supremum is taken over all prior distributions \(\Pi\).
\end{proposition}

\begin{proof}
Obviously, we have 
\[
\int_{\Theta} R(\hat{\theta}, \theta) \, \Pi(d\theta) \leq \sup_{\theta \in \Theta} R(\hat{\theta}, \theta).
\]
On the other hand, by using the prior distributions \(\delta_{\theta}\) (Dirac measure on \(\theta \in \Theta\)), we obtain
\[
\sup_{\Pi} \int_{\Theta} R(\hat{\theta}, \theta) \, \Pi(d\theta) \geq \int_{\Theta} R(\hat{\theta}, \theta) \, \delta_{\theta}(d\theta) = R(\hat{\theta}, \theta).
\]
\end{proof}

[Note: In the following we use the notation $\delta$ for decision rules while on the blackboard we used $\hat{\theta}$ or $\tilde{\theta}$. If you want to adjust this please contact me so that I can give you access.]

\begin{proposition}
Let \(\pi\) be a prior on \(\Theta\) such that
\[
R_{\pi}(\delta_{\pi}) = \sup_{\theta \in \Theta} R(\delta_{\pi}, \theta),
\]
where \(\delta_{\pi}\) is a \(\pi\)-Bayes rule. Then it holds that
\begin{enumerate}
    \item The rule \(\delta_{\pi}\) is minimax.
    \item If \(\delta_{\pi}\) is unique Bayes, then it is unique minimax.
\end{enumerate}
\end{proposition}

\begin{proof} Let \(\delta\) be any decision rule. Then
\[
\sup_{\theta \in \Theta} R(\delta, \theta) \geq \mathbb{E}_{\pi}[R(\delta, \theta)],
\]
\[
\int_{\Theta} R(\delta, \theta) \pi(\theta) \, d\theta \geq \mathbb{E}_{\pi}[R(\delta, \theta)],
\]
\[
\int_{\Theta} R(\delta, \theta) \pi(\theta) \, d\theta = R_{\pi}(\delta_{\pi}) = \sup_{\theta \in \Theta} R(\delta_{\pi}, \theta).
\]
Taking the infimum over \(\delta\) gives the result.

2. If \(\delta_{\pi}\) is unique Bayes, the second inequality is strict for any \(\delta' \neq \delta_{\pi}\).
\end{proof}

\begin{corollary}
If a (unique) Bayes rule \(\delta_{\pi}\) has constant risk in \(\theta\), then it is (unique) minimax.
\end{corollary}

\begin{proof}
If a Bayes rule \(\delta_{\pi}\) has constant risk, then
\[
R_{\pi}(\delta_{\pi}) = \mathbb{E}_{\pi}[R(\delta_{\pi}, \theta)] = \sup_{\theta \in \Theta} R(\delta_{\pi}, \theta),
\]
where \(R(\delta_{\pi}, \theta)\) is constant in \(\theta\). Uniqueness of the Bayes rule implies uniqueness of the minimax rule, as in part 2 of the former proposition.
\end{proof}

\begin{example}
Hence, if the maximal risk of a Bayes rule \(\delta_{\pi}\) equals the Bayes risk, then \(\pi\) is least favorable, and the corresponding Bayes rule is minimax.
\begin{itemize}
    \item In a \(\text{Bin}(n, \theta)\) model, let \(\pi_{a,b}\) be a \(\text{Beta}(a, b)\) prior on \(\theta \in [0,1]\). Then the unique Bayes rule for \(\pi_{a,b}\) over the quadratic risk is the posterior mean \(\delta_{a,b} = \bar{\theta}_{a,b}\). Trying to solve the equation
    \[
    R(\delta_{a,b}, \theta) = \text{const.} \quad \forall \theta \in [0,1]
    \]
    we can find a prior \(\pi_{a^*, b^*}\) and a corresponding Bayes rule \(\delta_{\pi_{a^*, b^*}}\) of constant risk. It is therefore unique minimax, and different from the MLE (see Examples sheet).
    
    \item In a \(\mathcal{N}(\theta, 1)\) model, \(\bar{X}_n\) is minimax, as proved later.
\end{itemize}
\end{example}

\section{Another optimality concept: Admissibility}

\begin{definition}
A decision rule \(\delta\) is \emph{inadmissible} if there exists \(\delta'\) such that
\[
R(\delta', \theta) \leq R(\delta, \theta) \quad \forall \theta \in \Theta \quad \text{and} \quad R(\delta', \theta) < R(\delta, \theta) \quad \text{for some } \theta \in \Theta.
\]
\end{definition}

\begin{remark}
\begin{itemize}
    \item The intuition is that there is no reason to choose an inadmissible estimator or decision rule: it would always be better to choose another estimator that dominates it.
    \item Admissibility is not the only criterion to evaluate an estimator: In most cases, a constant estimator will be admissible for the quadratic risk, but it is often not reasonable.
\end{itemize}
\end{remark}

\begin{proposition}
\begin{enumerate}
    \item A unique Bayes rule is admissible.
    \item If \(\delta\) is admissible and has constant risk, then it is minimax.
\end{enumerate}
\end{proposition}

Proof may be done in the Examples sheet.

\begin{definition}
For a vector \(X \in \mathbb{R}^p\), the \emph{James–Stein estimator} is defined as
\[
\delta^{JS}(X) = \left(1 - \frac{p - 2}{\|X\|^2}\right) X.
\]
\end{definition}

In a Gaussian model \(X \sim \mathcal{N}(\theta, I_p)\) for \(\theta \in \mathbb{R}^p\) (with a single observation, to simplify notation), the risk of the MLE is given by
\[
R(\hat{\theta}_{\text{MLE}}, \theta) = \mathbb{E}_{\theta}[\|X - \theta\|^2] = \sum_{j=1}^p \mathbb{E}_{\theta}[(X_j - \theta_j)^2] = p.
\]

For \( X \sim \mathcal{N}(\theta, I_p) \) with \( p \geq 3 \), the risk of the James–Stein estimator satisfies for all \( \theta \in \mathbb{R}^p \)
\[
R(\delta^{JS}, \theta) < p.
\]

\chapter{Confidence Sets}

\begin{definition}[Confidence Set]
Let \((\mathcal{X}, \mathcal{F}, (\mathbb{P}_{\theta})_{\theta \in \Theta})\) be a statistical model. For a given \(\alpha \in [0,1]\), consider sets \( C_{1-\alpha}(x) \subseteq \Theta \) for each \( x \in \mathcal{X} \). Then \( C_{1-\alpha}(x) \) is called a random confidence set at level \( 1 - \alpha \) (or with coverage probability \( 1 - \alpha \)) if
\[
\forall \theta \in \Theta : \mathbb{P}_{\theta}(\theta \in C_{1-\alpha}) = \mathbb{P}_{\theta}( \{ x \in \mathcal{X} : \theta \in C_{1-\alpha}(x) \}) \geq 1 - \alpha.
\]
\end{definition}

\textbf{Note:} The following example was only started in Lecture 4 and may be fully covered in Lecture 5.

\begin{example}
Consider the statistical model \((\{0,1\}^n, \mathcal{P}(\{0,1\}^n), (\mathbb{P}_p)_{p \in [0,1]})\) with \(\mathbb{P}_p = \text{Ber}(p)^{\otimes n}\) and independent observations \(X_k \sim \text{Ber}(p)\), \( k \in \{0, \ldots, n\}\). We are looking for a confidence interval \( C_{1-\alpha} \) around \( \hat{p} = \overline{X}_n = \frac{1}{n} \sum_{k=1}^n X_k \), i.e.,
\[
C_{1-\alpha} = [\overline{X}_n - a, \overline{X}_n + b] \quad \left(C_{1-\alpha}(x) = [\overline{X}_n(x) - a(x), \overline{X}_n(x) + b(x)] \right)
\]
should satisfy (where \(a\) and \(b\) might be random) that
\[
1 - \alpha \leq \mathbb{P}_p \left( p \in C_{1-\alpha} \right) = \mathbb{P}_p \left( \overline{X}_n - a \leq p \leq \overline{X}_n + b \right) = \mathbb{P}_p \left(-b \leq \overline{X}_n - p \leq a \right).
\]

Let \(t \mapsto F_p^n(t) := \mathbb{P}_p \left( \overline{X}_n - p \leq t \right)\) be the distribution function. Then,
\[
\mathbb{P}_p \left(-b \leq \overline{X}_n - p \leq a \right) = \mathbb{P}_p \left( \overline{X}_n - p \leq a \right) - \mathbb{P}_p \left( \overline{X}_n - p < -b \right)
\]
\[
= F_p^n(a) - F_p^n(-b) + R_n,
\]
where \( R_n = \mathbb{P}_p \left( \overline{X}_n - p = -b \right) \). Choose \(a, b\) as quantiles of \(\mathbb{P}_p^n\), i.e., \(a = \left(F_p^n\right)^{-1}(1 - \alpha / 2)\) and \( -b = \left(F_p^n\right)^{-1}(\alpha / 2)\) (with quantile function \(t \mapsto \left(F_p^n\right)^{-1}(t) := \inf \{t \in \mathbb{R} : F_p^n(t) \geq t \}\)).

However, \(F_p^n\) and thus \(a, b\) are unknown. Consider two possibilities:

\paragraph{Normal Approximation.} It holds that \( \mathbb{E}_p^n[X_k] = p \), \( \sigma := \text{Var}_p^n(X_k) = p(1 - p) \). By the central limit theorem, we have
\[
\frac{\sqrt{n}}{\sigma} \left( \overline{X}_n - p \right) = \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{X_k - p}{\sigma} \xrightarrow{d} N(0,1), \quad n \to \infty.
\]

For \( Z \sim N(0,1) \), it holds that
\[
F_p^n(a) = \mathbb{P}_p^n \left( \overline{X}_n - p \leq a \right) = \mathbb{P}_p^n \left( \frac{\sqrt{n}}{\sigma} \left( \overline{X}_n - p \right) \leq \frac{\sqrt{n}}{\sigma} a \right) \approx \mathbb{P}(|Z| \leq \frac{\sqrt{n}}{\sigma} a)
\]
\[
= \Phi \left( \frac{\sqrt{n}}{\sigma} a \right) = \Phi(z_\beta)
\]
for \( a := \frac{\sigma}{\sqrt{n}} z_\beta \) (where \( z_\beta \) is the \( \beta \)-quantile of the \( N(0,1) \)-distribution, i.e., \( \Phi(z_\beta) = \beta \)). In particular, \( R_n = o(1) \) (i.e., \( R_n \to 0 \) as \( n \to \infty \)). For \( a = b \) (since the \( N(0,1) \)-distribution is symmetric) and because \( \Phi(-x) = 1 - \Phi(x) \), it follows that
\[
\mathbb{P}_p^n(p \in C_{1-\alpha}) = F_p^n(a) - F_p^n(-a) + R_n \approx \Phi(z_\beta) - (1 - \Phi(z_\beta)) + o(1) = 2 \Phi(z_\beta) - 1 + o(1).
\]

For \( \beta = 1 - \alpha/2 \), \( C_{1-\alpha} \) is an asymptotically correct confidence interval. However, \( p \) and therefore \( \sigma \) and \( a \) are unknown. Solutions:

\begin{itemize}
    \item Estimate \( \sigma = p(1 - p) \leq 1/4 \) to widen the confidence interval.
    
    \item For the empirical variance \( \hat{\sigma}^2 = \frac{1}{n} \sum_{k=1}^n (X_k - \overline{X}_n)^2 \), we have \( \hat{\sigma}^2 \to \sigma^2 \) almost surely (by the law of large numbers). Using Slutsky’s lemma (Lemma: For random variables \( (X_n, Y_n)_{n \geq 1} \) with \( X_n \xrightarrow{d} X \), \( Y_n \xrightarrow{p} c \in \mathbb{R} \) (where \( c \) is deterministic), it holds that \( X_n + Y_n \xrightarrow{d} X + c \) and \( X_n \cdot Y_n \xrightarrow{d} c \cdot X \)), it follows that
    \[
    \frac{\sqrt{n}}{\hat{\sigma}} \left( \overline{X}_n - p \right) = \frac{\sigma}{\hat{\sigma}} \cdot \frac{\sqrt{n}}{\sigma} \left( \overline{X}_n - p \right) \xrightarrow{d} N(0,1), \quad n \to \infty.
    \]
\end{itemize}

From this, we derive \( a = \frac{\hat{\sigma}}{\sqrt{n}} z_{1 - \alpha / 2} \) (randomly chosen).

\[
\mathbb{P}_p^n(p \in C_{1 - \alpha}) = \mathbb{P}_p^n \left( |\overline{X}_n - p| \leq a \right) = \mathbb{P}_p^n \left( \left| \frac{\sqrt{n}}{\hat{\sigma}} (\overline{X}_n - p) \right| \leq z_{1 - \alpha / 2} \right)
\]
\[
\approx \mathbb{P}(|Z| \leq z_{1 - \alpha / 2}) = 2 \Phi(z_{1 - \alpha / 2}) - 1 = 1 - \alpha.
\]
\end{example}

\section{Hypothesis Testing}

\subsection{Basic Definitions}
Let \( (P_\theta : \theta \in \Theta) \) be a statistical model, and let \(\Theta = \Theta_0 \cup \Theta_1\) be a partition:
\begin{itemize}
    \item A \textbf{statistical test} is a measurable function of the data \( \varphi : (\mathcal{X}, \mathcal{F}) \to [0,1] \).
    \item If \( \varphi(x) \in \{0,1\} \) for all \( x \in \mathcal{X} \), then \( \varphi \) is a \textbf{non-randomized test}; otherwise, it is \textbf{randomized}.
    \item \( H_0 : \theta \in \Theta_0 \) is the \textbf{null hypothesis}.
    \item \( H_1 : \theta \in \Theta_1 \) is the \textbf{alternative hypothesis}.
    \item The map \( \theta \to \beta_\varphi (\theta) = P_\theta (\varphi = 1) \) is called the \textbf{power function} of a test \( \varphi \).
\end{itemize}

\subsection{Type I and Type II Errors}
\begin{itemize}
    \item For \(\theta \in \Theta_0\), \( \beta_\varphi(\theta) \) represents the \textbf{Type I error} (wrongly rejecting the null).
    \item For \(\theta \in \Theta_1\), \(1 - \beta_\varphi(\theta)\) represents the \textbf{Type II error} (failing to reject the alternative when it is true).
\end{itemize}

\[
1 \quad \quad \beta_\varphi(\theta) \quad 0 \quad \quad \Theta_0 \quad \quad \Theta_1 \quad \quad \Theta
\]

\textbf{Note:}
\[
1 - P_\theta (\varphi = 1) = P_\theta (\varphi = 0) = P_\theta \text{ (wrongly accepting the null)}
\]

\subsection{Level and Uniformly Most Powerful Tests}
\begin{definition}[Level]
A test \( \varphi : \mathcal{X} \to [0,1] \) has \textbf{level} \( \alpha \in [0,1] \) if
\[
\sup_{\theta \in \Theta_0} \beta_\varphi (\theta) \leq \alpha.
\]
\end{definition}

\begin{definition}[Uniformly Most Powerful Test]
Given a level \( \alpha \in (0,1) \), \( \varphi : \mathcal{X} \to [0,1] \) is called \textbf{uniformly most powerful (UMP)} if, for every other test \( \varphi' \) of level \( \alpha \) and all \( \theta \in \Theta_1 \),
\[
\beta_\varphi (\theta) \geq \beta_{\varphi'}(\theta).
\]
\end{definition}

\subsection{The Neyman-Pearson Lemma}
The Neyman-Pearson Lemma provides a basis for constructing the most powerful tests for simple hypotheses:
\begin{theorem}[Neyman-Pearson Lemma]
Let \( \Theta_0 = \{\theta_0\}\) and \( \Theta_1 = \{\theta_1\}\) be simple hypotheses:
\begin{enumerate}
    \item \textbf{Existence:} There exists a test \( \varphi \) and a constant \( k \in [0, \infty) \) such that \( P_{\theta_0} (\varphi = 1) = \alpha \), with
    \[
    \varphi(x) = 
    \begin{cases}
      1, & \text{if } \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} > k \\
      0, & \text{if } \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} < k
    \end{cases}
    \]
    Here, \( p_{\theta_1} \) and \( p_{\theta_0} \) are densities with respect to some dominated measure \( \mu \).
    
    \item \textbf{Sufficiency:} If \( \varphi \) satisfies \( P_{\theta_0} (\varphi = 1) = \alpha \) and the above form, then \( \varphi \) is a UMP level \( \alpha \) test.
    
    \item \textbf{Necessity:} If \( \varphi_k \) is UMP for level \( \alpha \), then it must be of the form shown above.
\end{enumerate}
\end{theorem}

\subsection{Proof of the Neyman-Pearson Lemma}
\begin{enumerate}
    \item Define the likelihood ratio \( r(x) = \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} \in [0, \infty) \). Let \( F_0 \) be the CDF of \( r(x) \) under \( P_{\theta_0} \).
    \[
    F_0(t) = P_{\theta_0} (r(x) \leq t).
    \]
    Define \( \alpha(t) = 1 - F_0(t) = P_{\theta_0} (r(x) > t) \) and note:
    \begin{itemize}
        \item \( \alpha \) is right-continuous:
        \[
        \lim_{\epsilon \to 0} \alpha(t + \epsilon) = P_{\theta_0} (r(x) > t).
        \]
        \item \( \alpha \) is non-increasing.
        \item \( \alpha \) has left limits.
    \end{itemize}

    \textbf{\( \alpha \) is cadlag:} It is continuous from the right and has a left limit.

    \[
    \text{There exists } k \in [0, \infty) \text{ such that }
    \alpha \leq \alpha(k^-) \quad \text{and} \quad \alpha \geq \alpha(k).
    \]

    We define the test
    \[
    \varphi(x) = 
    \begin{cases} 
      1 & \text{if } r(x) > k, \\
      \gamma & \text{if } r(x) = k, \\
      0 & \text{if } r(x) < k.
    \end{cases}
    \]
    
    Set \(\gamma = \frac{\alpha - \alpha(k)}{\alpha(k^-) - \alpha(k)}\).

    The level of \( \varphi \) is
    \[
    E_{\theta_0} [\varphi(x)] = P_{\theta_0} (\varphi(x) = 1).
    \]
    \[
    = P_{\theta_0} (r(x) > k) + P_{\theta_0} (r(x) = k) \cdot \gamma = \alpha.
    \]
\end{enumerate}
\chapter{Lecture 6: Neyman-Pearson Lemma and Likelihood Ratio Tests}
\section*{Neyman-Pearson Lemma}

\subsection*{Power of a Test}
The \textbf{power} of a test is defined as:
\[
E_{\theta_1}[\varphi] = P_{\theta_1}(\varphi = 1)
\]

\subsection*{Likelihood Ratio Test}
The \textbf{likelihood ratio} is given by:
\[
\Lambda(x) = \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} = r(x)
\]

\subsection*{Likelihood Ratio (LR) Test}
The LR test is defined as:
\[
\varphi(x) = 
\begin{cases} 
1 & \text{if } r(x) > k, \\
\gamma & \text{if } r(x) = k, \\
0 & \text{if } r(x) < k,
\end{cases}
\]
where \( k \in [0, \infty) \) and \( \gamma \in [0, 1] \).

\textbf{Note:} LR tests are Uniformly Most Powerful (UMP) for simple hypothesis testing:
\begin{itemize}
    \item Given a significance level \( \alpha \), if the LR test satisfies \( E_{\theta_0}[\varphi] = \alpha \), it controls the Type I error.
    \item The LR test minimizes the Type II error:
    \[
    E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi'] \quad \forall \varphi'
    \]
\end{itemize}

\subsection*{Continuation of Proof (Part of UMP)}
Let \( \varphi' \) be another level \( \alpha \) test such that \( E_{\theta_0}[\varphi'] \leq \alpha \).

\textbf{Goal:} Show that \( E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi'] \).

Let \( \mu \) be the dominating measure. Consider:
\[
\int (\varphi(x) - \varphi'(x)) \left( p_{\theta_1}(x) - k p_{\theta_0}(x) \right) d\mu(x) = 0
\]
\textbf{Claim:} \( p \geq 0 \).

\textbf{Observation:}
\begin{itemize}
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) > 0 \), then \( \frac{p_{\theta_1}(x)}{p_{\theta_0}(x)} > k \), implying \( \varphi(x) = 1 \).
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) < 0 \), then \( \varphi(x) = 0 \).
    \item If \( p_{\theta_1}(x) - k p_{\theta_0}(x) = 0 \), then the integrand is \( 0 \).
\end{itemize}
Thus, \( p = 0 \), leading to:
\[
\int (\varphi - \varphi') p_{\theta_1} \, d\mu = \int (\varphi - \varphi') p_{\theta_0} \, d\mu = k \left[ E_{\theta_0}[\varphi] - E_{\theta_0}[\varphi'] \right] \geq 0
\]
Therefore,
\[
E_{\theta_1}[\varphi] \geq E_{\theta_1}[\varphi']
\]

\subsection*{Part (3) UMP \(\Rightarrow\) LR}
Assume \( \varphi^* \) is a UMP test with \( E_{\theta_0}[\varphi^*] = \alpha \). Let \( \varphi \) be the LR test satisfying \( E_{\theta_0}[\varphi] = \alpha \).

\textbf{Goal:} Show that \( \varphi = \varphi^* \) almost everywhere except on \( \{ r(x) = k \} \).

Define the sets:
\[
x^+ = \{ x : \varphi(x) > \varphi^*(x) \}, \quad 
x^- = \{ x : \varphi(x) < \varphi^*(x) \}, \quad
x^0 = \{ x : \varphi(x) = \varphi^*(x) \}
\]
\[
\tilde{x} = (x^+ \cup x^-) \cap \{ x : p_{\theta_1}(x) \neq k p_{\theta_0}(x) \}
\]
It suffices to show \( \mu(\tilde{x}) = 0 \).

On \( \tilde{x} \):
\[
(\varphi - \varphi^*) (p_{\theta_1} - k p_{\theta_0}) > 0
\]
If \( \mu(\tilde{x}) > 0 \), then:
\[
\int_{\mathcal{X}} (\varphi - \varphi^*)(p_{\theta_1} - k p_{\theta_0}) \, d\mu \geq 0
\]
\[
\int_{\tilde{x}} (\varphi - \varphi^*)(p_{\theta_1} - k p_{\theta_0}) \, d\mu \geq 0
\]
However,
\[
E_{\theta_1}[\varphi] - E_{\theta_1}[\varphi^*] > k \left[ E_{\theta_0}[\varphi] - E_{\theta_0}[\varphi^*] \right] \geq 0
\]
This leads to a contradiction, implying \( \mu(\tilde{x}) = 0 \) and thus \( \varphi = \varphi^* \) almost everywhere.

\subsection*{Example: Gaussian Location Model}
Consider:
\[
X_1, \dots, X_n \overset{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)
\]
Testing:
\[
H_0 : \mu = \mu_0 \quad \text{vs.} \quad H_1 : \mu = \mu_1, \quad \mu_0 < \mu_1
\]
The likelihood ratio is:
\[
\frac{p_1(X_1, \dots, X_n)}{p_0(X_1, \dots, X_n)} = \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_1)^2 + \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2 \right)
\]
Simplifying:
\[
= \exp \left( -\frac{n}{2\sigma^2} (\mu_1^2 - \mu_0^2) - \frac{2(\mu_1 - \mu_0)}{\sigma^2} \sum_{i=1}^n X_i \right) \geq K_\alpha
\]
This implies:
\[
\frac{1}{n} \sum_{i=1}^n X_i \geq K_\alpha, \quad \text{for some } K_\alpha \in \mathbb{R}
\]
To determine \( K_\alpha \):
\[
\bar{X}_n := \frac{1}{n} \sum X_i \overset{H_0}{\sim} \mathcal{N}\left(\mu_0, \frac{\sigma^2}{n}\right)
\]
Thus:
\[
P_{H_0} \left( \bar{X}_n \geq K_\alpha \right) = 1 - \Phi \left( \frac{\sqrt{n}}{\sigma} (K_\alpha - \mu_0) \right)
\]
Solving for \( K_\alpha \):
\[
K_\alpha = \mu_0 + \frac{\sigma}{\sqrt{n}} \Phi^{-1}(1 - \alpha)
\]
Therefore, the LR test is:
\[
\varphi(X_1, \dots, X_n) = 
\begin{cases} 
1 & \text{if } \bar{X}_n \geq \mu_0 + \frac{\sigma}{\sqrt{n}} \Phi^{-1}(1 - \alpha), \\
0 & \text{otherwise}.
\end{cases}
\]

\section*{Corollary}

Consider simple hypothesis testing. Let \( \varphi \) be a UMP test at level \( \alpha \). Then:
\[
\alpha = E_{H_0}[\varphi] \leq E_{\theta_1}[\varphi]
\]
Suppose \( E_{\theta_1}[\varphi] = E_{\theta_1}[\varphi_0] \). Then \( \varphi_0 \) is also UMP, implying \( \varphi_0 \) is an LR test:
\[
\varphi_0 = 
\begin{cases} 
1 & \text{if } \frac{p_{\theta_1}}{p_{\theta_0}} \geq K \quad \text{a.s., for some } K, \\
0 & \text{otherwise}.
\end{cases}
\]
Since \( \varphi_0 \in \{\varphi, \beta\} \), it follows that \( p_{\theta_1} = K p_{\theta_0} \) almost surely.

Moreover:
\[
\int p_{\theta_0} \, d\mu = K \int p_{\theta_0} \, d\mu = 1 \quad \Rightarrow \quad K = 1
\]

\subsection*{Correspondence Theorem}

\textbf{Statement:} There is a correspondence between tests and confidence regions.

\[
\text{Tests} \quad \longleftrightarrow \quad \text{Confidence regions } C(x)
\]
with
\[
\Pr_{\theta}(\theta \in C(x)) \geq 1 - \alpha
\]
and
\[
\Pr_{\theta}(\phi_{\theta}(x) = 1) = \alpha
\]

\textbf{Theorem:} Let \( \{P_\theta : \theta \in \Theta\} \) be a statistical model and \( \alpha \in (0, 1) \).

\begin{enumerate}
    \item[(i)] If \( C = C(X) \) is a level-\(\alpha\) confidence set, then
    \[
    \phi_{\theta_0}(x) = \mathbb{I}\left\{ \theta_0 \notin C(x) \right\}
    \]
    is a level-\(\alpha\) test for \( H_0: \theta = \theta_0 \) vs. \( H_1: \theta \neq \theta_0 \).
    
    \item[(ii)] If \( \{\phi_{\theta} : \theta \in \Theta \} \) is a family of level-\(\alpha\) tests, then
    \[
    C(X) = \left\{ \theta \in \Theta : \phi_{\theta}(X) = 0 \right\}
    \]
    is a \( 1 - \alpha \) confidence set.
\end{enumerate}

\textbf{Proof:}
\begin{enumerate}
    \item[(i)] 
    \[
    \Pr_{\theta_0}(\phi_{\theta_0} = 1) = \Pr_{\theta_0}(\theta_0 \notin C(X)) \leq \alpha
    \]
    
    \item[(ii)] 
    \[
    \Pr_{\theta}(\theta \notin C(X)) = \Pr_{\theta}(\phi_{\theta}(X) = 1) \leq \alpha
    \]
\end{enumerate}

\section*{UMPT Tests in Models with Monotone Likelihoods}

\textbf{Proposition:} Let \( \Theta \subseteq \mathbb{R} \). Consider testing:
\[
H_0 : \theta \leq \theta_0 \quad \text{vs.} \quad H_1 : \theta > \theta_0,
\]
for some \( \theta_0 \in \mathbb{R} \).

Assume there exists a test statistic \( T : X \to \mathbb{R} \) and a function \( h : \mathbb{R} \times \Theta \times \Theta \to \mathbb{R} \) such that:
\[
\frac{P_{\theta}(X)}{P_{\tilde{\theta}}(X)} = h(T(X), \theta, \tilde{\theta})
\]
and for all \( \theta \geq \tilde{\theta} \), the function \( t \mapsto h(t, \theta, \tilde{\theta}) \) is monotone increasing.

\textbf{Conclusion:} LR tests are also UMP for level \( \alpha \). Specifically, the LR test of \( H_0: \theta = \theta_0 \) against \( H_1: \theta = \theta_1 \) for any \( \theta_1 > \theta_0 \) will be UMP.

\chapter{Linear Models}

\section{Introduction to Linear Regression Models}

Linear regression models are fundamental tools in statistical analysis, enabling us to understand and quantify the relationship between a dependent variable and one or more independent variables. In this lecture, we will explore the simplest form of linear regression, discuss the statistical framework underpinning it, and delve into key estimation techniques and their optimality properties.

\section{Simple Linear Regression Model}

Consider the simplest scenario where we model the relationship between a dependent variable \( Y_i \) and an independent variable \( X_i \) using a linear relationship:

\begin{equation}
    Y_i = aX_i + b + \varepsilon_i
    \label{eq:simple_linear_model}
\end{equation}

for \( i = 1, \ldots, n \), where:
\begin{itemize}
    \item \( a \) and \( b \) are unknown parameters representing the slope and intercept, respectively.
    \item \( \varepsilon_i \) is the error term, assumed to be centered, i.e., \( E(\varepsilon_i) = 0 \), and having constant variance \( \operatorname{Var}(\varepsilon_i) = \sigma^2 \).
\end{itemize}

We further assume that the error terms are normally distributed, \( \varepsilon_i \sim N(0, \sigma^2) \), with \( \sigma \) known.

\subsection{Statistical Model}

The statistical model can be formally defined as:

\[
(\mathbb{R}, \mathcal{B}(\mathbb{R}), \left(\bigotimes_{i=1}^n N(aX_i + b, \sigma^2)\right)_{(a,b) \in \mathbb{R}^2})
\]

Here:
\begin{itemize}
    \item \( \mathbb{R} \) denotes the real line.
    \item \( \mathcal{B}(\mathbb{R}) \) is the Borel sigma-algebra on \( \mathbb{R} \).
    \item \( \bigotimes_{i=1}^n N(aX_i + b, \sigma^2) \) represents the product measure of the normal distributions for each observation.
\end{itemize}

\subsection{Likelihood Function}

Within this statistical model, the likelihood function is given by:

\[
L((a,b) \mid y) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(y_i - aX_i - b)^2}{2\sigma^2} \right)
\]

Simplifying, we obtain:

\[
L((a,b) \mid y) = (2\pi \sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - aX_i - b)^2 \right)
\]

\subsection{Maximum Likelihood Estimation (MLE)}

The maximum likelihood estimators (MLE) for the parameters \( a \) and \( b \) are obtained by maximizing the likelihood function \( L((a,b) \mid y) \). Equivalently, since the logarithm is a monotonically increasing function, we can maximize the log-likelihood:

\[
\log L((a,b) \mid y) = -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - aX_i - b)^2
\]

Maximizing the log-likelihood is equivalent to minimizing the sum of squared residuals:

\[
(\hat{a}, \hat{b}) = \arg \min_{(a,b) \in \mathbb{R}^2} \sum_{i=1}^n (y_i - aX_i - b)^2
\]

Provided that \( X_i \neq X_j \) for \( i \neq j \), the least squares problem has a unique solution, historically attributed to Gauss (1801), given by:

\[
(\hat{a}, \hat{b}) = \left( \frac{\frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2}, \, \bar{Y} - \hat{a} \bar{X} \right)
\]

where \( \bar{X} \) and \( \bar{Y} \) denote the sample means of \( X \) and \( Y \), respectively.

\section{General Linear Models}

To generalize the simple linear regression model, we introduce the framework of linear models in multiple dimensions.

\begin{definition}[Linear Model]
    A random vector \( Y = (Y_1, \ldots, Y_n)^T \in \mathbb{R}^n \) stems from a linear model if there exists a parameter vector \( \beta \in \mathbb{R}^p \), a matrix \( X \in \mathbb{R}^{n \times p} \), and a random vector \( \varepsilon \in \mathbb{R}^n \) such that
    \[
    Y = X \beta + \varepsilon
    \]
    \begin{enumerate}
        \item \textbf{Regular Linear Model}: A linear model is called \emph{regular} if
        \begin{enumerate}
            \item The number of parameters does not exceed the sample size, i.e., \( p \le n \).
            \item The design matrix \( X \) has full rank, \( \operatorname{rank}(X) = p \le n \), ensuring a unique solution.
            \item The error vector satisfies \( E(\varepsilon) = 0 \), meaning the noise is centered.
            \item The covariance matrix of the errors is positive definite, \( \Sigma = \operatorname{Cov}(\varepsilon_i, \varepsilon_j)_{i,j \in [n]} \).
        \end{enumerate}
        \item \textbf{Ordinary Linear Model}: A linear model is called \emph{ordinary} if \( \Sigma = \sigma^2 I_n \), where \( I_n \) is the \( n \times n \) identity matrix. Typically, the noise is assumed to be Gaussian in this case.
    \end{enumerate}
\end{definition}

\begin{remark}
    \begin{enumerate}
        \item \textbf{Terminology:} Various synonyms are used in the literature:
        \begin{itemize}
            \item \( Y \): Dependent variable, response, regressand.
            \item \( X \): Independent variable, predictor, design matrix, regressor.
            \item \( \varepsilon \): Error, perturbation, regression function.
        \end{itemize}
        \item \textbf{Covariance Matrix \( \Sigma \):} The matrix \( \Sigma \) is symmetric and diagonalizable, i.e., \( \Sigma = UDU^T \) for some orthogonal matrix \( U \) and diagonal matrix \( D = \text{diag}(\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^{n \times n} \).
        \item \textbf{Positive Semi-definiteness:} The covariance matrix \( \Sigma \) is positive semi-definite, meaning \( \lambda_i \ge 0 \) for all \( i \), which can be shown as:
        \[
        \langle \Sigma u, u \rangle = \langle E[(\varepsilon - E[\varepsilon])(\varepsilon - E[\varepsilon])^T]u, u \rangle = E[(\varepsilon - E[\varepsilon])^2] \ge 0, \quad \forall u \in \mathbb{R}^n
        \]
        \item \textbf{Positive Definiteness and Inverses:} If \( \Sigma \) is positive definite (i.e., \( \lambda_i > 0 \) for all \( i \)), then its inverse \( \Sigma^{-1} \) and square root \( \Sigma^{-1/2} \) exist and can be expressed as:
        \[
        \Sigma^{-1} = UD^{-1}U^T \quad \text{and} \quad \Sigma^{-1/2} = UD^{-1/2}U^T
        \]
        \item \textbf{Random Design:} If the matrix \( X \) is not deterministic but random, the model is referred to as having a \emph{random design}.
    \end{enumerate}
\end{remark}

\section{Least Squares Estimation}

In the context of a regular linear model, the least squares estimator (LSE) seeks to minimize the weighted sum of squared residuals. Specifically, the LSE \( \hat{\beta} \) satisfies:

\[
\|\sigma^{-1/2}(Y - X\hat{\beta})\|^2 = \inf_{\beta \in \mathbb{R}^p} \|\sigma^{-1/2}(Y - X\beta)\|^2 = \inf_{\beta \in \mathbb{R}^p} \|\Sigma^{-1/2}Y - X_\Sigma \beta\|^2
\]

where \( X_\Sigma = \Sigma^{-1/2} X \).

\subsection{Geometric Interpretation}

The estimator \( X_\Sigma \hat{\beta} \) represents the point within the subspace:

\[
U = \{ X_\Sigma \beta \mid \beta \in \mathbb{R}^p \} \subseteq \mathbb{R}^n
\]

that is closest to the vector \( \Sigma^{-1/2} Y \) in terms of Euclidean distance. Formally, this can be expressed using the orthogonal projection \( \Pi_U \) onto \( U \):

\[
X_\Sigma \hat{\beta} = \Pi_U(\Sigma^{-1/2} Y)
\]

The orthogonal projection satisfies:
\begin{itemize}
    \item \( \Pi_U u = u \) for all \( u \in U \).
    \item \( \langle \Pi_U v - v, u \rangle = 0 \) for all \( u \in U \) and \( v \in \mathbb{R}^n \).
\end{itemize}

Provided that \( (X_\Sigma^T X_\Sigma)^{-1} \) exists, we can confirm by direct computation that the projection operator \( \Pi_U \) is given by:

\[
\Pi_U = X_\Sigma (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T
\]

For any \( u = X_\Sigma \beta \), we have:

\[
X_\Sigma (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T X_\Sigma \beta = X_\Sigma \beta = u
\]

By symmetry, for any \( u \in U \) and \( v \in \mathbb{R}^n \):

\[
\langle \Pi_U v - v, u \rangle = \langle v, \Pi_U u \rangle - \langle v, u \rangle = \langle v, u \rangle - \langle v, u \rangle = 0
\]

\section{Representation for the Least Squares Estimator}

\begin{lemma}[Representation for the LSE]
    Consider a regular linear model. Then the least squares estimator (LSE) exists uniquely and is given by:
    \[
    \hat{\beta} = (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T \Sigma^{-1/2} Y = X_\Sigma^+ \Sigma^{-1/2} Y
    \]
    where \( X_\Sigma^+ \) denotes the Moore-Penrose pseudoinverse of \( X_\Sigma \).
\end{lemma}

\begin{proof}
    Since \( X \) has full rank and \( p \le n \), the matrix \( X_\Sigma^T X_\Sigma \) is invertible. Suppose \( v \in \ker(X_\Sigma^T X_\Sigma) \), then:

    \[
    0 = v^T X_\Sigma^T X_\Sigma v = (X_\Sigma v)^T (X_\Sigma v) = \| X_\Sigma v \|^2 = \| \Sigma^{-1/2} X v \|^2
    \]

    This implies \( \| X v \|^2 = 0 \), hence \( X v = 0 \). Given that \( X \) has full rank, the only solution is \( v = 0 \). Therefore, \( X_\Sigma^T X_\Sigma \) is invertible.

    The projection of \( \Sigma^{-1/2} Y \) onto \( U \) is:

    \[
    X_\Sigma \hat{\beta} = \Pi_U(\Sigma^{-1/2} Y) = X_\Sigma (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T \Sigma^{-1/2} Y
    \]

    Multiplying both sides by \( X_\Sigma^T \):

    \[
    X_\Sigma^T X_\Sigma \hat{\beta} = X_\Sigma^T X_\Sigma (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T \Sigma^{-1/2} Y = X_\Sigma^T \Sigma^{-1/2} Y
    \]

    Hence, solving for \( \hat{\beta} \):

    \[
    \hat{\beta} = (X_\Sigma^T X_\Sigma)^{-1} X_\Sigma^T \Sigma^{-1/2} Y
    \]
\end{proof}

\begin{remark}
    \begin{enumerate}
        \item If \( p > n \), the matrix \( X_\Sigma^T X_\Sigma \) is not invertible, and the LSE is not unique. In this case, the set of solutions is a \( (p - n) \)-dimensional subspace, and each solution interpolates the data perfectly, i.e.,
        \[
        \{ \beta \in \mathbb{R}^p \mid \| \Sigma^{-1/2} Y - X_\Sigma \beta \|^2 = 0 \}
        \]
    \end{enumerate}
\end{remark}

\section{Optimality of the Least Squares Estimator}

The least squares estimator possesses several optimality properties under the ordinary linear model. This is formalized in the Gauss-Markov Theorem.

\begin{theorem}[Gauss-Markov Theorem]
    Consider an ordinary linear model with \( \sigma > 0 \). Then:
    \begin{enumerate}
        \item The least squares estimator \( \hat{\beta} = (X^T X)^{-1} X^T Y \) is a linear and unbiased estimator for the parameter \( \beta \).
        \item For any desired linear combination of parameters \( \alpha = \langle \beta, v \rangle \) where \( v \in \mathbb{R}^p \), the estimator \( \hat{\alpha} = \langle \hat{\beta}, v \rangle \) is the best linear unbiased estimator (BLUE) of \( \alpha \). This means that \( \hat{\alpha} \) has the smallest variance among all linear unbiased estimators of \( \alpha \).
        \item The estimator \( \hat{\sigma}^2 = \frac{\| Y - X \hat{\beta} \|^2}{n - p} \) is an unbiased estimator of \( \sigma^2 \).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item \textbf{Linearity and Unbiasedness of \( \hat{\beta} \):}
        
        The estimator \( \hat{\beta} \) is linear because it can be expressed as a linear transformation of \( Y \):
        \[
        \hat{\beta} = (X^T X)^{-1} X^T Y
        \]
        For any two vectors \( y \) and \( \tilde{y} \) in \( \mathbb{R}^n \),
        \[
        \hat{\beta}(y + \tilde{y}) = (X^T X)^{-1} X^T (y + \tilde{y}) = \hat{\beta}(y) + \hat{\beta}(\tilde{y})
        \]
        demonstrating linearity.

        To show unbiasedness, compute the expectation of \( \hat{\beta} \):
        \begin{align*}
            E[\hat{\beta}] &= (X^T X)^{-1} X^T E[Y] \\
            &= (X^T X)^{-1} X^T E[X \beta + \varepsilon] \\
            &= (X^T X)^{-1} X^T (X \beta + E[\varepsilon]) \\
            &= (X^T X)^{-1} X^T X \beta \quad \text{(since \( E[\varepsilon] = 0 \))} \\
            &= \beta
        \end{align*}
        Hence, \( \hat{\beta} \) is unbiased.

        \item \textbf{Optimality of \( \hat{\alpha} \) as BLUE:}

        Consider a linear unbiased estimator \( \tilde{\alpha} \) for \( \alpha = \langle \beta, v \rangle \). Since \( \tilde{\alpha} \) is linear, there exists a vector \( w \in \mathbb{R}^n \) such that:
        \[
        \tilde{\alpha} = \langle Y, w \rangle
        \]
        For \( \tilde{\alpha} \) to be unbiased, we require:
        \[
        E[\tilde{\alpha}] = \langle E[Y], w \rangle = \langle X \beta, w \rangle = \langle \beta, X^T w \rangle = \alpha = \langle \beta, v \rangle
        \]
        This implies that:
        \[
        v = X^T w
        \]
        The variance of \( \tilde{\alpha} \) is:
        \[
        \operatorname{Var}(\tilde{\alpha}) = \operatorname{Var}(\langle Y, w \rangle) = \operatorname{Var}(\langle X \beta + \varepsilon, w \rangle) = \operatorname{Var}(\langle \varepsilon, w \rangle) = \sigma^2 \| w \|^2
        \]
        Now, consider the variance of \( \hat{\alpha} = \langle \hat{\beta}, v \rangle \):
        \begin{align*}
            \operatorname{Var}(\hat{\alpha}) &= \operatorname{Var}(\langle \hat{\beta} - \beta, v \rangle) \\
            &= \operatorname{Var}\left( \langle (X^T X)^{-1} X^T \varepsilon, v \rangle \right) \\
            &= \operatorname{Var}\left( \langle \varepsilon, X (X^T X)^{-1} v \rangle \right) \\
            &= \sigma^2 \| X (X^T X)^{-1} v \|^2 \\
            &= \sigma^2 \| X (X^T X)^{-1} X^T w \|^2 \quad \text{(since \( v = X^T w \))} \\
            &= \sigma^2 \| \Pi_U w \|^2 \\
            &\le \sigma^2 \| w \|^2 \quad \text{(since projection does not increase the norm)}
        \end{align*}
        Therefore, \( \operatorname{Var}(\hat{\alpha}) \le \operatorname{Var}(\tilde{\alpha}) \), showing that \( \hat{\alpha} \) has the smallest variance among all linear unbiased estimators of \( \alpha \).

        \item \textbf{Unbiasedness of \( \hat{\sigma}^2 \):}

        The estimator \( \hat{\sigma}^2 \) is given by:
        \[
        \hat{\sigma}^2 = \frac{\| Y - X \hat{\beta} \|^2}{n - p}
        \]
        To show that \( \hat{\sigma}^2 \) is unbiased, compute its expectation:
        \begin{align*}
            E[\hat{\sigma}^2] &= \frac{E[\| Y - X \hat{\beta} \|^2]}{n - p} \\
            &= \frac{E[\| \varepsilon \|^2 - \| X \hat{\beta} \|^2 + 2 \langle \varepsilon, X \hat{\beta} \rangle]}{n - p} \\
            &= \frac{E[\| \varepsilon \|^2]}{n - p} \quad \text{(since \( E[\langle \varepsilon, X \hat{\beta} \rangle] = 0 \))} \\
            &= \frac{n \sigma^2}{n - p} \quad \text{(since \( \| \varepsilon \|^2 \) is chi-squared with \( n \) degrees of freedom)} \\
            &= \sigma^2 \quad \text{(since \( E[\| \varepsilon \|^2] = n \sigma^2 \))}
        \end{align*}
        Hence, \( \hat{\sigma}^2 \) is an unbiased estimator of \( \sigma^2 \).
    \end{enumerate}
\end{proof}

\begin{remark}
    \begin{enumerate}
        \item The Gauss-Markov theorem establishes that under the ordinary linear model assumptions, the least squares estimator is the best (in the sense of having the smallest variance) among all linear unbiased estimators. This optimality holds without requiring the error terms to be normally distributed.
    \end{enumerate}
\end{remark}

\section{Conclusion}

In this lecture, we have introduced the fundamental concepts of linear regression models, both in their simplest form and in a more general framework. We discussed the estimation of model parameters using the method of least squares, explored the geometric interpretation of the estimator, and established its optimality through the Gauss-Markov theorem. Understanding these foundational elements is crucial for further studies in statistical modeling and inference.



\chapter{Lecture 8}

Recall the linear model:
\[
Y = X\beta + \varepsilon
\]
where $\text{Cov}(\varepsilon) = \Sigma$.

\paragraph{OLD Estimator}
\[
\hat{\beta} = \left(X_\Sigma^T X_\Sigma\right)^{-1} X_\Sigma^T \Sigma^{-1/2} Y.
\]

\paragraph{Projection Interpretation}
\[
X\hat{\beta} = \text{Projection of } \Sigma^{-1/2} Y \text{ onto the span } \{X_{\varepsilon,1}, \hdots, X_{\varepsilon,p}\}.
\]

\begin{theorem}[Gauss-Markov]
\begin{enumerate}
    \item $\hat{\beta}_{\text{OLS}}$ is the Best Linear Unbiased Estimator (BLUE).
    \item For any linear combination $\alpha_i = \langle \beta, v \rangle$, the estimator $\hat{\alpha}_i$ is BLUE.
    \item The estimator 
    \[
    \hat{\sigma}^2 = \frac{||Y - X\hat{\beta}||^2}{n - p}
    \]
    is an unbiased estimator for $\sigma^2 > 0$.
\end{enumerate}
\end{theorem}

Expressed component-wise, the model is:
\[
\begin{pmatrix}  
    y_1  \\  
    y_2 \\ 
    \vdots \\ 
    y_n  
\end{pmatrix}
=
\begin{pmatrix}  
    x_1  \\  
    x_2 \\ 
    \vdots \\ 
    x_n  
\end{pmatrix} 
\begin{pmatrix}  
    \beta_1  \\  
    \beta_2 \\ 
    \vdots \\ 
    \beta_p  
\end{pmatrix}^T 
+
\begin{pmatrix}  
    \varepsilon_1  \\  
    \varepsilon_2 \\ 
    \vdots \\ 
    \varepsilon_n  
\end{pmatrix}
\]
where our data is $(Y_i, X_i)_{i=1}^n \in (\mathbb{R} \times \mathbb{R}^p)^{\otimes n}$.

\begin{remark}
Is this an IID model? It depends!
\begin{enumerate}
    \item Typically, $\varepsilon_i$ are IID.
    \item If $X_i$ are random, then we have a "random design".
    \item If $X_i$ are IID, then the linear model is an IID model.
    \item If $X_i$ are deterministic, then it is not an IID model.
\end{enumerate}
\end{remark}

Consider the mapping:
\[
\beta \mapsto ||Y - X\hat{\beta}||.
\]

\begin{proof}[Proof of Theorem (Point 3)]
This proof continues from point 3 of the Gauss-Markov theorem.

We have already introduced the projection matrix:
\[
\Pi_U = X(X^T X)^{-1}X^T
\]
which projects onto the column space $U$ of $X$. Thus, $I_n - \Pi_U$ is the projection operator onto $U^\perp$, the orthogonal complement of $U$:
\[
U^\perp = \left\{ z \in \mathbb{R}^n \mid \langle z, X_k \rangle = 0 \text{ for all } k = 1, \hdots, p \right\}.
\]
Choose an orthonormal basis $e_1, \hdots, e_{n-p}$ of $U^\perp$. Then,
\[
(I_n - \Pi_U)z = \Pi_{U^\perp} z = \sum_{k=1}^{n-p} \langle z, e_k \rangle e_k.
\]
Now, compute the norm:
\begin{align*}
||Y - X\hat{\beta}||^2 &= ||Y - \Pi_U Y||^2 \\
&= ||(I_n - \Pi_U) Y||^2 \\
&= ||(I_n - \Pi_U)(X\beta + \varepsilon)||^2 \\
&= ||(I_n - \Pi_U)\varepsilon||^2 \\
&= \sum_{k=1}^{n-p} \langle \varepsilon, e_k \rangle^2.
\end{align*}
Taking expectation:
\[
\mathbb{E}\left[ ||Y - X\hat{\beta}||^2 \right] = \sum_{k=1}^{n-p} \mathbb{E}\left[ \langle \varepsilon, e_k \rangle^2 \right] = (n - p) \sigma^2.
\]
Hence, 
\[
\mathbb{E}\left[ \hat{\sigma}^2 \right] = \frac{\mathbb{E}\left[ ||Y - X\hat{\beta}||^2 \right]}{n - p} = \sigma^2.
\]
\end{proof}

\begin{remark}
Recall the $N(\mu, \sigma^2)$ model, where the Maximum Likelihood Estimators (MLE) are:
\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} Y_i, \quad \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{\mu})^2.
\]
The unbiased estimator for $\sigma^2$ is:
\[
\frac{1}{n - 1} \sum_{i=1}^{n} (Y_i - \hat{\mu})^2,
\]
which relates to the $n - p$ factor in point 3 of the Gauss-Markov theorem.
\end{remark}

\begin{remark}
\begin{enumerate}
    \item If linearity is dropped, there exist better estimators than $\hat{\beta}_{\text{OLS}}$. For example, a constant estimator $\hat{\beta} = \beta^*$.
    \item The Mean Squared Error (MSE) of $\hat{\beta}_{\text{OLS}}$ is:
    \[
    \mathbb{E}\left[ ||\hat{\beta}_{\text{OLS}} - \beta||^2 \right] = \mathbb{E}\left[ \sum_{i=1}^{p} \langle \hat{\beta}_{\text{OLS}} - \beta, e_i \rangle^2 \right] = \sum_{i=1}^{p} \text{Var}_\beta\left( \langle \hat{\beta}_{\text{OLS}}, e_i \rangle \right) = \sum_{i=1}^{p} \sigma^2 ||X(X^T X)^{-1} e_i||^2.
    \]
    We say $X$ satisfies an orthogonal design if:
    \[
    X^T X = nI_p.
    \]
    This implies that the different covariates are uncorrelated, i.e., $(X^T X)_{ij} = \langle X_i, X_j \rangle = n \delta_{ij}$.
    
    For an orthogonal design, the MSE simplifies to:
    \[
    \mathbb{E}_\beta\left[ ||\hat{\beta}_{\text{OLS}} - \beta||^2 \right] = \frac{\sigma^2 p}{n}.
    \]
    This expression represents the noise level multiplied by the number of parameters, divided by the number of data points.
\end{enumerate}
\end{remark}

\begin{theorem}[Bayes in Linear Models]
Consider the linear model $Y = X\beta + \varepsilon$, where $\varepsilon \sim N(0, \sigma^2 I_n)$ with $\sigma > 0$ known, and assume a prior distribution $\beta \sim N(m, M)$ where $m \in \mathbb{R}^{p}$ and $M \in \mathbb{R}^{p \times p}$ is positive semi-definite. Then, the posterior distribution $\Pi(\beta \mid Y, X)$ is given by:
\[
\Pi(\beta \mid Y, X) = N(\mu_{\text{post}}, \Sigma_{\text{post}})
\]
where
\[
\mu_{\text{post}} = \Sigma_{\text{post}} \left( \sigma^{-2} X^T Y + M^{-1} m \right), \quad \Sigma_{\text{post}} = \left( \sigma^{-2} X^T X + M^{-1} \right)^{-1}.
\]
\end{theorem}

\begin{remark}
$\Sigma_{\text{post}}$ is independent of $Y$. As $M^{-1} \to 0$, the posterior mean $\mu_{\text{post}} \to \hat{\beta}_{\text{OLS}}$.
\end{remark}

\begin{proof}
The posterior is proportional to the product of the likelihood and the prior:
\[
L(X, Y, \beta) \pi(\beta) \propto \exp\left( -\frac{1}{2\sigma^2} ||Y - X\beta||^2 - \frac{1}{2} (\beta - m)^T M^{-1} (\beta - m) \right).
\]
We want to express this in the form of a Gaussian distribution:
\[
\exp\left( -\frac{1}{2} (\beta - \mu_{\text{post}})^T \Sigma_{\text{post}}^{-1} (\beta - \mu_{\text{post}}) \right).
\]
Expanding the exponents:
\begin{align*}
& -\frac{1}{2\sigma^2} (Y - X\beta)^T (Y - X\beta) - \frac{1}{2} (\beta - m)^T M^{-1} (\beta - m) \\
&= -\frac{1}{2} \beta^T \left( \frac{1}{\sigma^2} X^T X + M^{-1} \right) \beta + \beta^T \left( \frac{1}{\sigma^2} X^T Y + M^{-1} m \right) + \text{const}.
\end{align*}
Completing the square, we identify:
\[
\Sigma_{\text{post}} = \left( \frac{1}{\sigma^2} X^T X + M^{-1} \right)^{-1}, \quad \mu_{\text{post}} = \Sigma_{\text{post}} \left( \frac{1}{\sigma^2} X^T Y + M^{-1} m \right).
\]
Thus, the posterior distribution is:
\[
\Pi(\beta \mid Y, X) = N(\mu_{\text{post}}, \Sigma_{\text{post}}).
\]
\end{proof}

\begin{corollary}
For the loss function $\ell(\beta) = ||\cdot||^2$, the Bayes estimator is the posterior mean:
\[
\hat{\beta}_\Pi = \mu_{\text{post}}.
\]
\end{corollary}

\begin{proposition}
Consider the previous setting from the theorem, with $m = 0$ and $M = \tau^2 I_p$ (a centered, isotropic normal prior). Then, the posterior mean $\mu_{\text{post}} = \hat{\beta}_\Pi$ minimizes:
\[
\beta \mapsto ||Y - X\beta||^2_{\mathbb{R}^n} + \frac{\sigma^2}{\tau^2} ||\beta||^2_{\mathbb{R}^p}.
\]
\end{proposition}

\begin{proof}
With $m = 0$ and $M = \tau^2 I_p$, the posterior mean is:
\[
\mu_{\text{post}} = \Sigma_{\text{post}} \left( \sigma^{-2} X^T Y \right), \quad \Sigma_{\text{post}} = \left( \frac{1}{\sigma^2} X^T X + \frac{1}{\tau^2} I_p \right)^{-1}.
\]
The objective function to minimize is:
\[
J(\beta) = ||Y - X\beta||^2 + \frac{\sigma^2}{\tau^2} ||\beta||^2.
\]
Taking the gradient with respect to $\beta$:
\[
\nabla_\beta J(\beta) = -2X^T(Y - X\beta) + 2\frac{\sigma^2}{\tau^2} \beta = 0.
\]
Solving for $\beta$:
\[
X^T X \beta + \frac{\sigma^2}{\tau^2} \beta = X^T Y \implies \left( X^T X + \frac{\sigma^2}{\tau^2} I_p \right) \beta = X^T Y.
\]
Thus, the minimizer is:
\[
\hat{\beta}_{\text{ridge}} = \left( X^T X + \frac{\sigma^2}{\tau^2} I_p \right)^{-1} X^T Y.
\]
Comparing with the posterior mean:
\[
\mu_{\text{post}} = \left( \frac{1}{\sigma^2} X^T X + \frac{1}{\tau^2} I_p \right)^{-1} \frac{1}{\sigma^2} X^T Y = \left( X^T X + \frac{\sigma^2}{\tau^2} I_p \right)^{-1} X^T Y = \hat{\beta}_{\text{ridge}}.
\]
Therefore, $\mu_{\text{post}}$ minimizes the given objective function.
\end{proof}

\begin{remark}
The estimator $\hat{\beta}$ is defined even if $\text{rank}(X) < p$, in particular, even for $n < p$.
\end{remark}

\begin{definition}[Ridge Regression]
The estimator $\hat{\beta}$ given by
\[
\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \left( ||Y - X\beta||^2_{\mathbb{R}^n} + \lambda ||\beta||_{\mathbb{R}^p}^2 \right)
\]
is called a ridge regression estimator. The parameter $\lambda > 0$ is referred to as the regularization parameter. We sometimes denote this estimator by $\hat{\beta}_{\text{ridge}}$. The ridge regression estimator $\hat{\beta}_{\text{ridge}}$ is always uniquely defined.
\end{definition}

\begin{proposition}[MSE of $\hat{\beta}_{\text{ridge}}$]
Consider a linear model with $\varepsilon \sim N(0, \sigma^2 I_n)$, $\sigma > 0$ known, and assume an orthogonal design where $X^T X = nI_p$. Let $\alpha = \langle \beta, v \rangle$ for some $v \in \mathbb{R}^p$, and let $\hat{\alpha}_{\text{ridge}} = \langle \hat{\beta}_{\text{ridge}}, v \rangle$. Then:
\begin{enumerate}
    \item 
    \[
    \mathbb{E}_\beta \left[ |\hat{\alpha}_{\text{ridge}} - \alpha|^2 \right] = \left(1 + \frac{n}{\lambda}\right) \langle \beta, v \rangle^2 + \frac{\sigma^2 ||v||^2}{n} \left(1 + \frac{\lambda}{n}\right).
    \]
    \item 
    \[
    \mathbb{E}_\beta \left[ ||\hat{\beta}_{\text{ridge}} - \beta||^2 \right] = \left(1 + \frac{n}{\lambda}\right)^{-2} ||\beta||^2 + \frac{p \sigma^2}{n} \left(1 + \frac{\lambda}{n}\right)^{-2}.
    \]
    The second term represents the risk of $\hat{\beta}_{\text{OLS}}$ scaled by $\left(1 + \frac{\lambda}{n}\right)^{-2}$.
\end{enumerate}
\end{proposition}



















\iffalse


\appendix

\numberwithin{definition}{section}
\numberwithin{theorem}{section}
\numberwithin{proposition}{section}
\numberwithin{lemma}{section}
\numberwithin{corollary}{section}
\numberwithin{remark}{section}
\numberwithin{example}{section}
\numberwithin{supplement}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Appendix to the chapters} \label{ch:appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introducing the problem}\label{sec: appendix to chap 1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

.....

\fi 

\medskip
\printbibliography[heading=bibintoc,title={References}] % you can change the title however you want
\end{document}